-- Slide Set 1 -- What is an OS -------------------------------------------------------------------

    What is an OS?
        - An operating system is a sftware layer that                                                 Applications      v
            + manages hardware resources                                                              Operating Systems -
            + provides an abstraction of the underlying hardware that is easier to program and use    Hardware          ^
        - An OS is not 
            + A compiler, Standard Libraries, Command Shells
            + While closely related, they are not part of the OS

    Benifits:
        - The operating system abstracts the hardware
            + details of the raw hardware are hidden so applications can be simplier & smaller
            + faster access to the hardware
            + application writers can program to a simpler and more portable "virtual machine"
        - Providing useful logical abstractions
            + new types of logical resources like sockets, pipes, and files systems
        - Protecting applications from one another
            + Enforce "fair" allocation of hardware resources among applications
            + Policies that say what is "fair: and mechanisms to enforce it
        - Supporting communication and coodination among applications
            + Support abstractions through wich different applications can share data and notify each other of events

    OS Issues: 
        - Concurencey 
            + How many types of activties can occor at once
        - Protection
            + What is granularity at which permissions to access various resources are granted
            + How do you verify an entity's right to access a resource
        - Fault Tolerance
            + How do we deal with faults in applications? In devices? In our own OS code?
        - Resource/services Provided to applications
            + Does the OS offer Kernal support for events? Signals? Threads? Pipes? Shared Memory?
        - Naming
            + How does the application refer to and request the resources they want for themselves? Resources they want to share with others?
        - Sharing
            + What objects can be shared among applications? What is the granularity of sharing?
        - Resource Allocation and Tracking
            + What is the unit[s] of resource allocation
            + Can we track (and bill for) resource usage?

    OS Goals: 
        - Abstract the raw hardware
        - Protect apps from one another
        - Not allow for applications to monopolize more than their fair share of system resources
        - Provide desired functionality
        - Expose the raw capability of the hardawre, minimizing the "tax"
        - Optimize for the expected workload
        - Be simple enough that the code executes quickly and can be debugged easily 
    
-- Slide Set 2 -- History -------------------------------------------------------------------------
    
    At first OSes were just shared libraries:
        - Each programmer didn't need to write a code to manage each devices
        - Each application when complied contained the "os"
        - Loaded memory by hand (by "operators") through mechanical switches
        - Just one application at a time so no need for protection or sharing
        - No virtual memory, application either fit into memory or it didn't or progammers moved sections in and out of memory by hand
    
    Batch Processing:
        - Still only 1 app at a time
        - OS, not operators, loaded jobs after one another using punch cards or tapes
        - OS knew how to read the next job in, execute it and take back control when doesnt
        - Operating system stayed in memopry permenantly
        - Spooling
            + Problem
                - Card readers are slow
                - time reading jobs from cards meant lost CPU time
            + Solution
                - Load next job into memory while executing current job
    
    Multiprogrammed Batch Systems:
        - keep multiple jobs in memory at the same time and take turns on executions rather than running to completion
            + Applications still cant communicate directly
        - Able to overlap I/O of one app with another
            + If one job requires I/O for on app, dont leave the CPU hanging, run something else
            + While jobs took longer, the CPU was better utilized (They were very expensive so that was important)
        - Requires some modern OS functionality: 
            + simple cpu scheduler, memory management, memory and I/O protection, and asyncronous I/O
            + Still far off though

    Time Sharing: (i.e. CTSS, Multics, and UNIX)
        - Interactive computing
            + Connect to computer via dumb terminal (monitor, keyboard, serial connection to computer)
            + Each interactive user fells like they have their own computer, but in reality, jobs are swapped on and off the CPU rapidly enough that users dont notice
            + Enables interactive applications like editors, command shells, and even debugging running programs
            + User interacts with jobs through its run time
        - Scheduling
            + Need to swap jobs on and off the CPU quickly so users dont notice
            + Each job is given a time slice
            + Batch scheduling was very difficult - let application run until it did some I/O then swap it out until its I/O is doesn
            + Batch optimizes for throughput; Timesharing optimizes for response time
        - Share File Systems
            + Users login over a dumb terminal over a serial lines
            + Command shells execute user command then wait for another
            + Thus time sharing systems needed shared file systems that held commonmly used programs
            + Users could login, run utilities, store inputs and output files in shared file systems
        - Security 
            + Batch only had applications and the inputs were fixed
            + Time Sharing means multiple interactive userts which leads to increased threats

    Personal Computers:
        - Computers cheap enough that one can be dedicated to an individual
        - First PC was the Altair in 1975
        - 1975 -> 1980, may companies make PCs (or microcomputers) based on the 8080 chip
            + Early computers mainly for hobbists, most run CP/M OS (Control Program Microcomputer)
            + Closer to Batch than Time Sharing
        - 1980, IBM gets into PC buisness
            + uses intel 8080 hardware and Micorsoft's BASIC interpreter but needed an OS
            + Microsoft purchaces QDOS (Quick and Dirty) and renamed it MS-DOS
                - library linked in with applications
                    + 1 M address space; apps only got 140K
                - No memory protection (apps could do anything they want)
                - No hierarical file system, single directory with at most 64 files
        Windows:    
            - 1985, Windows 1.0
                + Runs as a library on top of DOS
                + allowed useres to switch between several programs without requireing them to quit and restart
            - 1987, Windows 2.0 
                + allows overlapping windows
            Split into 2 lines
                - 1994, Windows NT
                    + entirely new OS kernal (not DOS) designed for server machines
                - 1995, Windows 95
                    + Included MS-DOS 7.0 but after startup it took over completely
                    + Pre-emptive multitasking, advanced files systems, threading, networking
                - 2000, Windows 2000
                    - upgrade to Windows NT code base
                    - Designed to replace 95 and its DOS roots
                - XP, Vista, 7, 8, 10, CE, Mobile
        Linux:
            - Created by a student based on a student operating systems called minux in 1991
            - worked with newsgroup to develop open source OS to create a UNIX style OS for PCs
            - Different distributions package the same Linux kernal with differnt open source softwares
        PC-OSs meet Timesharing:
            - Both Linux and later versions of windows brought advanced OS concepts to the desktop
                + Multiprogramming for user multitasking
                + Memory protection to protect against buggy applications
                + PC-OSs allowed users to login remotely and multiple users to be running jobs
            - Steady increase of hardware perfomance made this possible

    Mobile/Cellphone OS:
        -Apple iOS, Google Android, and Microsoft's Windows Phone OS
            + Android uses Linux, iOS based off of MacOS
        - Similar evolution to personal computers
            + First one app at a time now can run amny apps at once
            + stripped down protection and multiprogramming
            + Dont log in as multiple users but for example Android installs each app as a different user for permissions and resources

    Pararllel and Distributed computing
        - Harness resources of multiple computer Systems
            + Pararllel computing focuses on splitting up asingle task and getting speedup proportional to the number of machines
            + Distributed computing focuses on harnessing resources from geographically dispersed machines
            + Cloud computing - using resources in datacenters located elsewhere rather than local resources

-- Slide Set 3 -- Processes ----------------------------------------------------------------------- 

    Programs vs Processes
        - A program is a sequence of commands waiting to be run
        - A process is active
            + An instance of a program being executed
            + There may be many processes running the same program
            + also called a job or a task

    - What makes up a process?
        + Address space
        + code
        + data
        + Stack (nesting of procedure calls made)
        + Register values (Including the PC)
        + Resources allocated to the process
            - Memory, open files network connections
        + Stack and heap grow

    Address Space Map:
        Biggest ^  +----------------------------------------+ <--- Sometimes reserved for OS
        Virtual |  | Stack - Space for local variables etc. |
        Address |  |         For each nested procedure call |
                |  +----------------------------------------+ <--- Stack Pointer
                |  |       |                  ^             | <--- Biggest area
                |  |       V                  |             |      OS will typically stop you long before the stack and heap overlap but it can be changed  
                |  +----------------------------------------+
                |  | Heap - Space for dynamically allocated |
                |  | memory (e.g. with malloc)              |
                |  +----------------------------------------+
                |  | Staticly Decalre Variables             |
                |  | (Global Variables)                     |
                |  +----------------------------------------+
                |  | The Code                               | <--- PC
                |  | Text segment                           |
        0x0000  V  +----------------------------------------+ <--- Sometimes reserved for Error Catching

    How is a process represented?
        - Usually by a process or task object
        - Process control Block
        - When not running, OS has to remember Registers, statistics, Working directory, Open Files, 
          Users who owns process, Timers, Parent Processes and Sibling process ids
        - In linux, task_struct defined in include/linux/sched.h
            + struct that store important info

    Management of PCBs (Process control block)
        - PCBs are datastructures
        - Space for them may be dynamically allocated as needed or perhaps a fixed size array of PCBs 
          for the maximum number of possible processes allocated at one time
        - As a process is created, a PCB is assigned and initialized for it
            + Often process id is an offet into an array of PCBs
        - After process Terminates, PCB is freed
            + Sometimes kept around for parent to retrive its exit status
    
    State Transition Diagram:
                         Scedule/Unschedule
        (New) -> (Ready) <----------------> (Running) -> (Terminated)
                    ^                           |
     Grant resource |                           | Request resource or service
                    +---- (Wating) <------------+

        New -> Ready          - Process created startes in Ready
        Ready -> Running      - chosen to be scheduled on a processer
        Running -> Terminated - Process exits or fails
        Running -> Waiting    - Process requests resources from the OS
        Running -> Ready      - Context switch off the CPU or yields the CPU
        Wating -> Ready       - Resources granted, process able to run again when chosen

    State Queues:
        - OSs often maintain a number of queues of processes that represent the state of the processes
            + All the runnable processes are linked together into one queue (or perhaps multiple based on priority)
            + All the processes blocked (or perhaps bloced for a particular class of event) are linked together
            + As a process changes state, it is unlinked from one queue into another

    Context Switch:
        - When a process is running, some of its state is stored directly in the CPU (Reg. values, etc.)
        - When the OS stops a process, it must save all of this hardware state somewhere (PCB) so that it can be restored again
        - The act of saving one processes hardware state and restoring another's is called a contex switch
            + 100s or 1000s a second

    CPU Scheduler:
        - Selects which process should be executed next and allocates CPU
        - Short term scheduler is invoked very frequently (milliseconds)

    Cooperating Processes:
        - Processes can run independantly of each other or processes can coordinate their activities with other processes
        - To cooperate, processes must use OS facilities to communicate
            + One example: parent process waits for a child
            + Others: Files, Sockets, Pipes...

    Signals:
        - Processes can register to handle signals with the signal
            + void signal (int signum, void (*proc) (int))
        - Processes can send signals with the kill function
            + kill (pidn signum)
        - System defined signals like SIGHUP (0), SIGKILL (9), SIGSEGV (11)
        - Signals not used by the system like SIGUSR1 and SIGUSR2

    Orphaned processes:
        - If a parent dies, child becomes a child of init
        - But if the shild shares file handles with parent there can be trouble
        - For a child to be an indepenedant adult ("demonized") it generally kills its parent
        
-- Slide Set 4 -- Kernel -------------------------------------------------------------------------- 

    Day in the life of an OS:
        - When a machine boots, the operating system will execute for some time to initalize the state of the machine
          to start up certain processes
        - Once initalizization is done, the OS only executes when some "event", like system call, occurs that requires 
          its attention
        - When an event occurs
            + The current state of the machine is saved
            + The mode changes to protected mode
            + An event handler procedure is executed (must have a handler for every event)

    Types of events:
        - software event (often called exceptions)
            + normal software requests for OS services are called "traps"
            + Software errors that transfer control to the OS are called "faults"
            + Sometimes system calls called software interrupts
        - Hardware events (e.g. device notifies CPU that it has completed an I/O request)
            + Sometimes say "trap to the OS" to handle hardware interrupt

    OS initalizization:
        - During the OS initalizization on INtel CPU:
            + Interupt Descriptor Table (IDT) loaded with handlers for wach kind of interupt
        - System call is interupt vector 128 (0x80)

    Regaining the CPU:
        - If a user level application is running on the CPU, what can the OS do to make it yeild the CPU after its turn
            + Timer (clock) operation
            + Timer generates interupts on a regular interval to transfer control back to the OS
    
    System Calls:
        - If an application legitimatley needs to access a protected feature, it calls a system call
            + System call instruction executed with a parameter that designates specific call and any other needed paremeters
            + The state of the user program is saved so that it can be restored (context switch to the OS)
            + Control is passed to an OS procedure to accomplish the task amd mode bit changed
            + OS procedure runs for the user program but can verify the programs "rights" and refuse to do it
            + On completion of the system call, the state of the user program including hr old mode bit is restored

    System Call Illustrated:
        File.open("/home/Readme")                               Resume application 
            |                                               with file opened or error
            V                                                           ^
        SystemCall(SYS_OPEN,"/home/Readme")   User mode                 |
            |                                                           |
        ----+-----------------------------------------------------------+------
            V                                                           |
        Save user registers and mode         Kernal Mode                |
        lookup SYS_OPEN in a table of                                   |
        call procedures, change mode bit,                               |
        jump to the kernelOpen procedure                                |
            |                                                   Restore user mode
            V                                                   and application's
        kernelOpen("/home/Readme", application has rights) ---> registers etc.

    Memory Protection:
        - All codes executes on the CPU must be loaded into its memory
            + Its executed by setting the program counter register to point to the location of the next instruction and execute
        - OS has its code in memory and so does each runnable user process
        - Give each process a contiguos set of memory addresses to use and deicate two registers to specifying the top
          and the bottom of this region
        - Memory protection hardware in reality is more powerful than base and limit registers (page tables, TLB, etc.)

        +-----------+                       When Process 1 is executing, Base and limit set to point to process 1's memory region
        |     OS    |                       if process 1 tries to load or store to addresses outside this region, then hardware
        +-----------+ <- Base Registers     will transfer control to the OS
        | Process 1 |
        +-----------+ <- Limit Resisters
        | Process 2 |
        +-----------+

-- Slide Set 5 -- Scheduling ---------------------------------------------------------------------- 

    Scheduler:
        - A scheduler is the module tahat moves jobs from queue to queue
        - Scheduler runs when
            + A process/thread blocks on a request
            + A timer interupt occurs
            + A new process/thread is created or is terminated

    Scheduling Algorithm:
        - The sceduling algorithm examines the set of candidate processes/threads and chooses one to execute
        - Sceduling algorithms can have different goals
            + Maximize CPU utilization
            + Maximize throughput
            + Minimize average turnaround time (Avg(EndTime - StartTime))
            + Minimize response time

    - Starvation is a process being prevented from making progress beacuse another process hasa resource it needs
        + Sceduling policies should try to prevent starvation

    Types of Scheduling:
        - To find best scheduler, think of a process/thread as an entity that alternates between two startes
          using the CPU and waiting for I/O
        - First Come First Serve (FCFS/FIFO)
            + Also called First in First Out
            + Jobs Sceduled in the order they arive
            + When used, tends to be non-premptive
                - If you get there first, you will get all the resources until youre done
                - "Done" can mean completion or end of CPU burst
            + Sounds Fair
                - All Jobs Treated Equaly
                - No Starvation (except for infinate loops that prevent completion)
            + Cons
                - Can Lead to poor overlap of I/O and CPU
                    + If left First in line will run until they are done or block for I/O then can get oncvoy effect
                    + While job with long CPU burst executes, other jobs complete their I/O and the I/O devices sit 
                      idle even though they are the bottleneck resource and should always be kept busy
                - Also small jobs wait behind long running jobs which results in high average turn around time

        - Short Jobs First (SJF)
            + To prevent short jobs waiting behind long ones, let the jobs with the shortest CPU burst go next
                - Cant prive that this results in the optimal average wait time
            + Can be preemptive or non-preemptive
                - preemptive ones called shortest remaining time first
            + Cons
                - Starvation for long running jobs because they may never get to the front of the queue
                - Can't really know if a job will have the shortest CPU burst

        - Most Important Job First
            + Priority scheduling
                - Assign priority to jobs and run the job with the highest priority first
                - Can be preemptive such that as soon as a high priority job arrives it gets the CPU 
            + Can implement with multiple "Priority Queues" instead of a single ready queue
                - Run all jobs on highest priority first
            + Cons
                - How are priorities decided?
                    + SJF is priority scheduling based on run time
                - Like SJF, starvation is possible
                    + Possible solution, increase priority based on wait time, 
                      eventually any job will accumulate enough "waiting points" to be scheduled
                - How do you schedule when 2 processes have the same priority
                - What if the highest priority needs the most resources 
                - Priority inversion
                    + lowest priority process holds a lock that the highest priority process needs.
                      medium priority processes can run while low priority never gets a chance to releases lock
                    + Solution is having the process with a lock be considered the highest priority process until 
                      it releases it, then it reverts back to its original priority

        - Round Robin (RR)
            + Sceduling broken into time slices
            + Each job gets its share of CPU for a slice of time
            + No starvation
            + Cons
                - How do you choose the size of the time slice?
                    + If too small, then spend all your time context switching and very little time making progress (Too much overhead)
                    + If too big, then every other process has to wait between the times a given job is sceduled leading to poor response time
                    + RR with too large time slice => FIFO
                - No way to express priority
                    + Some jobs should get longer slices

        - Best of all Worlds?
            + To create a realistic sceduling algorithm, combine elements from several of the basic schemes to ballance demands
                + Have multiple queues
                + Use different algorithms for different queues
                + Use different algorithms between queues
                + Have algorithms for moving jobs between queues
                + Have different time slices for each queue
                + Where do new jobs enter the system

        - Multi-level Feedback Queue (MLFQ)
            + Multiple queues representing different types of jobs
                - I/O bound, CPU bound
                - Queues have different priorities
            + Jobs can move between queues based on execution history
            + If any job can be guarenteed to make it to the top priority queue given enough time, then its starvation freed
                - If enough time in low priority queue raises its effective priority

            + Example MLFQ
                - 3-4 classes spanning > 100 priority levels
                    + timesharing, interactive, system, realtime (highest)
                - Processes with the highest priority always run first; the same priority scheduled Round Robin
                - Reward interactive behavior by increasing priority if process blocks before its time slice is up
                - Punnish CPU hogs by decreasing priority of process uses its whole time slice
        
    nice:
        - users can lower the priority of processes with nice
        - root user can raise or lower the priority of the process
    
-- Slide Set 6 -- Threads -------------------------------------------------------------------------  

    Why use > 1 sequential process:
        - some problems are hard to solve as a single sequential process; easier to express as a colection of cooperating processes
            + Hard to write code to manage many different tasks all at once
            + Think of it like making a phone call while doing the dishes while looking through the mail
            + Can't be indepenedent processes becuase shared data (brain) and shared resources (the kitchen and phone)
            + Can't do them sequentially because need to make progress all at once
            + Easier to write an algorithm for each and when there is a lull in one activity let the OS swicth between them
        - Example: Webserver
            + Listen for incoming socket requests
                - Once it receives a request, it ignores listening to the incoming socket while it does the request
                - Must do both at once
            + A solution would be to create a child process to handle the request and allow the parent to return to listening
                - This is inefficient due to address space creation (and memory usage) and PCB initalizization
            + There are similarities between the processes that are spawned off to handle requests
                - They sahre code, have the same privileges, share the same resources to return, cgi script to run, database to search
            + But there are differences
                - Operating on different requests
                - Each one will be in different stages of the handle request algorithm
            + Solution
                - Let these tasks share the address space, privileges, and resources
                - Give each their own registers (like the PC) their own stack etc.

    single threaded process                  Multi threaded process
    +------+------+-------+   +-----------+-----------+-----------+
    | code | data | files |   |   code    |   code    |   code    |
    +------+------+-------+   +-----------+-----------+-----------+
    | registers   | stack |   | registers | registers | registers |
    +-------------+-------+   |   stack   |   stack   |   stack   |
    |      thread         |   +-----------+-----------+-----------+
    |                     |   |  thread 1 |  thread 2 |  thread 3 |
    +---------------------+   +-----------+-----------+-----------+

    Processes vs Threads:
        - Each thread belongs to one process
        - One process may contain multiple threads
        - Threads are the logical unit of scheduling
            + PC, stack, local variables            
        - Processes are the logaical unit of resource allocation
            + address space, privileges, resources

    Address Space Map for multithreaded program:
        Biggest ^  +----------------------------------------+ 
        Virtual |  | Thread 1 Stack - Variable declaration  |
        Address |  +----------------------------------------+ <--- Stack Pointer (Thread 1)
                |  |       |                       ^        |
                |  |       V                       |        |
                |  +----------------------------------------+
                |  | Thread 2 Stack - Variable declaration  |
                |  +----------------------------------------+ <--- Stack Pointer (Thread 2)
                |  |       |                  ^    |        | 
                |  |       V                  |    |        | 
                |  +----------------------------------------+
                |  | Heap - Space for dynamically allocated |
                |  | memory (e.g. with malloc)              |
                |  +----------------------------------------+
                |  | Staticly Decalre Variables             |
                |  | (Global Variables)                     |
                |  +----------------------------------------+
                |  | The Code                               | <--- PC (Thread 2)
                |  | Text segment                           | <--- PC (Thread 1)
        0x0000  V  +----------------------------------------+ 

    Warning: threads can be dangerous!
        - One can argure (and John Ousterhout did) that threads are a bad idea for most purposes
        - Anything you can do with threads you can do with an event loops
            + Remember "Make making phone call while doing dishes and looking at mail"
        - Ousterhout said thread programming is hard to get right
        - "Although threads seem to be a small step from sequential computation, 
          in fact, they represent a huge step. They discard the most essential 
          and appealing properties of sequential computation: understandability, 
          predictability, and determinism. Threads, as a model of computation, are 
          wildly nondeterministic, and the job of the programmer becomes one of pruning 
          that nondeterminism." 
                -- 'The Problem with Threads, Edward A. Lee

    Kernel support for threads:
        - Some OSes support the notion of multiple threads per processes and others do not
        - Each user level thread can map to a kernel level thread (Called 1:1)
        - When switch between threads don't have to flush TLB - do still have context switch overhead
        - Even if no "kernel threads" can build threads at user level
            + Each "multithreaded program gets a single kernel in the process
            + During its timeslice, it runs code from its various threads

    User level threads:
        - User level thread switch must be programmed in assembly (restore values to registers, etc.)
        - User level thread packages avoid having one thread monopolize the process timeslice by managing them like 
          the OS manages processes on the CPU
            + Solution 1: non-premptive
                - Rely on each thread to periodically yeild
                - yeild would call the sceduling function of the library
            + Solution 2: OS is to user level thread package like hardware is to OS
                - Ask OS for periodic timer signal
                - Use that to gain control and switch the running thread

    Kernel vs User level:
        - One might think that kernel level are best and only if kernel soes not support threads use user level threads
        - In fact, user level threads can be much faster 
            + Thread Creation, "Context Switch" between threads, communication between threads all done at user level
            + Procedure calls instead of system calls (verification of all user arguments, etc.) in all these cases
        - Kernel level threads are slow due to having to swtich to kernel mode every switch 
          (having to go through the process of saving etc.)
            + Kernel level thread creation and joining - 94 ms
            + User levele thread creation and joining - 4.5 ms
        - With kernel level threads, kernel chooses among all possible threads to scedule
        - With user level threads , kernel scedules the process and the user level thread package scedules the thread
        - User level threads have benefit of fast context switch at user level
        - Kernel level threads have benifit of global knowledge of sceduling choices and has more flexibility in assigning 
          priorities to individual threads

    Problems with user level threads:
        - OS does not ahve information about thread activity and can make bad sceduling decisions
        - Examples
            + If thread blocks, the whole process blocks
                - Kernel threads can take overlap I/O and computation within a process
            + Kernel may scedule a process with all idle threads
        - Possible solution: Scheduler Activations (M:N)
            + If have user level thread support availible then use kernel threads *and* user-level threads
            + Each Process requests a number of kernel threads to use for running user-level threads on
            + Kernel procises to tell user level before it blocks a kernel thread so user-level thread package can choose what to do with the remaining kernel threads
            + User level promises to tell kernel when it no longer needs a given kernel level thread

-- Slide Set 7 -- Syncronization ------------------------------------------------------------------ 
 
    Concurency (Exeucting processes out of order) is a good thing:
        - So far we have mostly talked about constructs to enable concurrency
            + Multiple processes, inter-process communication
            + Multiple threads in a process
        - Concurencey critical to using the hardware devices to full capacity
            + Always something that needs to be running on the CPU, using eac device, etc.
        - We dont want to restrict concurency unless we absolutely have to (When to restrict concurencey)
            + Some resoruce so heavily utilized that no one is getting any benifit from their small piece
                - Too many processes wanting to use the CPU (while(1) fork())
                - "Thrashing"
                - Solution: Access Control (Starvation?)
            + Two processes/threads we would like to execute concurrently are going to access the same data
                - One writing the data while the other is reading; two writing over top at the same time
                - Solution: Synchronization (Deadlock?)
                - Synchronization primitives enable safe concurencey

    Syncronization Required:
        - Required for all shared data structures like:
            + Shared Databases
            + Global Variables
            + Dynamically allocated structures (off the heap) like queues, lists, trees, etc.
            + OS data structures like running queue, the process table
        - What are not shared data structures?
            + Variables local to the procedure (on the stack)
            + Other bad things that happen if try to share pointer to a variable that is local to the procedure

    Critical Section Problem:
        - Model processes/threads as alternating between code that accesses shared data (critical section) and code that doesn not (remainder section)
            do {
                ENTRY SECTION
                    critical section
                EXIT SECTION
                    remainder section
            }
        - ENTRY SECTION requests access to the shared data, EXIT SECTION notifies of completion of the critical section
        - Solution:
            + Mutual Exclusion
                - Only one process is allowed to be in its critical section at once
                - All other processes forced to wait on ENTRY
                - When one process leaves, another may enter
            + Progress
                - If a process is in the critical section, it should not be able to stop another process from entering it indefinately
                - Decision of who will be next cant be delayed indefinately
                - Cant give one process access; cant deny access to everyone
            + Bounded Waiting
                - After a process has made a request to enter its critical section, there should be a bound on the number 
                  of times other processes can enter their critical section

    Syncronization Primitives:
        - Used to implement a solution to the critical section problem
        - OS uses Hardware Primitives
            + Hardware test and set
            + Disable interrupts
        - OS exports primitives to user apps; User level can build more complex primitives from simpler OS primitives 
          and some HW primitives
            + Locks
            + Semaphores
            + Monitors
            + Messages

    Hardware Test and Set:
        - Atomic operation that reads current value of a register, writes 1 to its location and returns the old value
        - If a process is running a test and set, no others may begin another until the first is finished

    Locks:
        - Object with two operations, lock and unlock
        - Threads use pairs of lock/unlock
            + Lock before entering a critical section
            + Unlock upon exiting a critical section
            + If a thread is in their critical section, then the lock will not return until it can be acquired
            + Between lock and unlock, a thread "holds" the lock
        - Lock -> run critical section -> unlock -> another thread ready to do critical section ->
        - Issues
            + Forget lock? No exclusive access
            + Forget unlock? Deadlock
            + Put it in the wrong place?
        - Implementing Locks
            + Lock has critical section itself (read lock; if free, write lock taken)
            + Need help from hardware
                - Make a basic lock primitive using atomic instructions (like test-and-set, read-modify-write)
                - Prevent context switched
                    + Disable interrupts
        - Spinlocks
            + Uses a while loop to wait for a the lock to unlock with a test-and-set
                - If the test-and-set returns 1, the lock is in use
                - If the test-and-set returns 0, the lock is unlocked
            + Wasteful of CPU time
                - thread spinning still uses its full share of the cpu sycles waiting, called busy waiting
                - During that time, thread holding the lock cannot make progress
                - Waht if thread waiting has higher priority tahn the treads holding the lock
            + Safe at user level but inefficient
        - OS can build lock so when lock is called, if the process or thread dosent acquire the lock, it is taken off the ready queue 
          into a special queue of processes that are waiting for the lock to be released
            + When the lock is released, the OS chooses a waiting process to grant the lock to an place it back into the ready queue
            + often called a semaphore

    Semaphores:
        - Just because comething is called a semaphore, doesnt make it one
            + something called a semephore could implement lock or test-and-set
        - Semaphore object has 2 data members: an int value and a queue of waiting processes
        - Uses operations wait and signal
            + Wait operation (like lock)
                - decrements the semephore's integer value and blocks the thread calling wait until the semephore is availible
                - also called P() after dutch word, proberen, to testg
            + Signal operation (like unlock)
                - Increments the semaphore's integer value and if threads are blocked waiting, allow onw to "enter" the semephore
                - Also called V() after dutch word, verhogen, to increment
            + Dutch because it was invented by a dutchman, Edgar Dykstra fot THE OS (strict layers) in 1968
        -Binary semephore
            + Semephor's value initialized to 1
            + used to guarentee exclusive access to shared resource (functionaly like a lock but without the busy waiting)
        - Counting semephore
            + Semephore's value initialized to N>0
            + Used to control access to a resource with N interchangable units availible (e.g. N processors)
            + Allow threads to enter a semephore as long as sufficient resources are availible
        - When value is > 0, semephore is open
            + thrad calling wait will continue
        - When value is <= 0, semephore is closed
            + thread wait will decrement value and block
        - When the value is negative, it tells how many threads are waiting on the semephore
        - If the OS exports the semephore, the kernel is aware of the waiting queue
        - If the user level thread package exports the semephore, then user level thread scheduler is aware of the waiting queue
        - Busy waiting is not gone completely
            + When accessing a semephore's critical section, thread holds the semephores lock and another process that tries 
              to call wait or signal at the same time will busy wait
            + Semephores critical section is normally much smaller than the section it is protecting, so the busy waiting is 
              greatly reduced
            + Boils down a long critical section into a short one - maximizes the time that parallelism is ok and minimize 
              busy waiting

    Are spin locks always bad?
        - Adaptive locking in solaris
        - Adaptive mutexes
            + Multiprocessor system if cant get lock
                - And thread with lock is not running, then sleep
                - And thread with lock is running, then spin wait
            + Uniprocessor if cant get lock
                - Immediatley sleep (no hope for lock to be released while its running)
        - Programmers choose adaptive mutexes for short coed segments and semephores or condition variables for longer ones
        - Blocked threads placed on a seperate queue for desired object
            + Thread to gain access next chosen by priority and priority inversion is implemented

-- Slide Set 8 -- Syncronization 2 ---------------------------------------------------------------- 
    
    Problems with locks and semephores:
        - No syntatic connection between lock/semephore and the shared data/resources being protected
            + Thus the meaning of the semephores is defined by the programmer's use of it
                - Semephores are basically global variables accessed by all threads
            + Easy for programmers to make a mistake
        - No seperation between use for mutual exclusion and use for resource management and use for expresing sceduling 
          constraints
        - Add languge support for synchronization
            + Delcare a section of code to require mutually exclusive access
            + Associate the shared data itself wit hthe locking automatically
        - A Monitor is programming language support to enforce yncronization
            + mutual exclusion code added by the comiler

    Monitors:
        - A monitor is a software module that encapsulates
            + Shared data structures
            + Procedures that operated on them
            + Synchronization required of processes that invoke these procedures
        - Like a public/private data interface prevents access to private data members; 
          monitors prevent unsynchronized access to shared data structures
        - Can implement semephores with monitors and monitors with semephores
    
    Condition Variables:
        - Have 2 operations, wait and signal (like semephores)
            + x.wait() mains the process invoking this operation is suspended until another process invokes x.signal()
            + The operation wait allows another process to enter the monitor
            + The x.signal operation resumes exactly 1 suspended process 
                -If no processes is suspended, then it has no effect (unlike semephores)
        - If thread monitor calls x.signal waking another thread, then whos is running on the monitor now?
            + Run akaened thread next; signaler blocks
            + Waiter is made ready; signaler continues
        - Can use without a monitor
            + Basicly just a semaphore without a history
            + Couldnt do locking with it beause no mutual exclusion on its own
            + Couldnt do resource management because no value/history
            + Can use it for ordering/sceduling constraints
        
    Events/Messages:
        - Expreses ordering between two actions
        - Windows Events
            + Synchronization objetcs used somewhat like semephores when they are used for ordering/scheduling constraints
            + One process/thread can wait for an event to be signaled by another process/thread
            + Create/destory
            + Wait
            + Signal(all threads that wait on it receive)
        - UNIX signals 
            + Can be used for synchronization
                - Signal handler sets a flag
                - Main thread polls on the value of the flag
                - Busy wait though

    Synchronization Primitives Summary:
        - Locks
            + Simple semantics, often close to hardware primitives
            + Can be inefficient if implemented as a spin lock
        - Semephores
            + Internal queue - more efficient if integrated with scheduler
        - Monitors
            + Language constructs that automate the locking
            + Easy to program with when supported by the language and where the model fits the task
            + Once condition variables added in, much of the ocmplexity comes back
        - Events/Messages
            + Simple model of synchronization via data sent over a channel

-- Slide Set 9 -- Classic Syncronization Problems ------------------------------------------------- 
 
    Bounded-Buffer Problem: (Producer/Consumer)
        - Finite size buffer (array) in memory shared by multiple processes/threads
        - Procuder threads "produce" an item and place it in the buffer
        - Consumer threads remove an item from the buffer and "consume" it
        - Why do we need synchronization
            + Shared data - the buffer state
            + Which parts of the buffer are free? Which filled?
        - What could go wrong?
            + Producer doesnt stop when no free spaces
            + Consumer tries to consume an empty space
            + Consumer tries to consume a space that is only half-filled by the producer
            + Two producers try to fill the same space
            + Two consumers try to consume the same space
        
        Semaphore Solution 1: (inneficeint)
            - initalize 3 semephores
                + Mutex (mutually exclusive access)
                + Full (if space is full)
                + Empty (is space is empty)
            - Producer 
                + waits until empty semephore somewhere
                + only goes to claim the mutex after it finds empty
                + waits for mutex to unlock once its found an empty space
                + produces for empty space when availible
                + signals mutex and full for that space
            - Consumer
                + waits until full semephore somewhere
                + only goes to claim the mutex after it finds full
                + waits for mutex to unlock once its found an Full space
                + produces for empty space when availible
                + signals mutex and empty for that space
            - Could be better
                + dont know how long to produce/consume
                + If it takes a long time, it becomes inneficeint
            Code:
                semaphore_t mutex;
                semaphore_t full;
                semaphore_t empty;

                container_t {
                    BOOL free = TRUE;
                    item_t   item;
                }
                container_t buffer[FIXED_SIZE];

                void initBoundedBuffer{
                    mutex.value = 1;
                    full.value = 0;
                    empty.value = FIXED_SIZE	
                }
                void producer (){
                    container_t *which;	
                    wait(empty);
                    wait (mutex);
                    
                    which = findFreeBuffer();
                    which->free = FALSE;
                    which->item = produceItem();
                    
                    signal (mutex);
                    signal (full);
                }
                void consumer (){
                    container_t *which;	
                    wait(full);
                    wait (mutex);
                    
                    which = findFullBuffer();
                    consumeItem(which->item);
                    which->free = TRUE;
                    
                    signal (mutex);
                    signal (empty);
                }

        Semaphore Solution 2:
            - Add lock bool to container
            - Producer
                + after mutex is unlocked, set current container to find a free unlocked buffer
                + set that buffer locked to true
                + signal the mutex
                + produce
                + set status to full
                + signal that there is a space full
            - Consumer
                + after mutex is unlocked, set current container to find a full unlocked buffer
                + set that buffer locked to true
                + signal the mutex
                + consume
                + set status to empty
                + signal that there is a space empty
            - Now it doesnt matter how long it takes to produce because that is locked per buffer
            Code:
                semaphore_t mutex;
                semaphore_t full;
                semaphore_t empty;

                container_t {
                    BOOL free = TRUE;
                    BOOL locked = FALSE;
                    item_t   item;
                }
                container_t buffer[FIXED_SIZE];

                void initBoundedBuffer{
                    mutex.value = 1;
                    full.value = 0;
                    empty.value = FIXED_SIZE	
                }

                void producer (){
                    container_t *which;	
                    wait(empty);
                    wait (mutex);
                    
                    which = findFreeUnlockedBuffer();
                    which->locked = TRUE;
                    signal (mutex);
                    
                    which->item = produceItem();
                    which->free = FALSE;
                    which->locked = FALSE;
                    
                    signal (full);
                }

                void consumer (){
                    container_t *which;	
                    wait(full);
                    wait (mutex);
                    
                    which = findFullUnlockedBuffer();
                    which->locked = TRUE;
                    signal (mutex);
                    
                    consumeItem(which->item);
                    which->free = TRUE;
                    which->locked = FALSE;
                    
                    signal (empty);
                }

        Monitor Solution: 
            - call monitor instead of mutexes
                + call condition variables 
                    - oneEmptied
                    - oneFilled
                + count numFull
            - Producer
                + while numFull is equal to size of array wait for oneEmptied
                + once empty is found, produce
                + add count to numFull
                + signal oneFilled
            -Consumer
                + while numFull is equal to 0 wait for oneFull
                + once full is found, consume
                + take one away from numFull
                + signal oneEmptied 
            Code: 
                container_t {
                BOOL free = TRUE;
                item_t   item;
                }

                monitor boundedBuffer {
                    conditionVariable oneEmptied;
                    conditionVariable oneFilled;
                    container_t buffer[FIXED_SIZE];
                    int numFull = 0;

                    void producer (){
                        while(numFull == FIXED_SIZE){
                        wait(oneEmptied)
                        }
                        
                        which = findFreeBuffer();
                        which->free = FALSE;
                        which->item = produceItem();
                        numFull++

                        signal(oneFilled);
                    }

                    void consumer (){
                        while(numFull ==0){
                        wait(oneFilled)
                        }
                        
                        which = findFullBuffer();
                        consumeItem(which->item);
                        which->free = TRUE;
                        numFull--;

                        signal(oneEmptied);
                    }
                }

    Reader/Writers:
        - Shared data area being accessed by multiple processes/threads
        - Reader threads look but dont touch
            + We can allow multiple readers at a time
        - Writer threads touch too
            + If a writer is present, no other writers and readers
        - Producer/Consumer is a subset of this
            + Producer/Consumer is a writer
            + While there is no reader, there could be a report buffer function
        - Fundamentally all synchronization problems are Reader/Writer problems

        - Reader's preferance
            + Uses okToWrite semephore and numReaders int
                - If numReaders = 1 then force writers to wait
                - If numReaders = 0 on exit signal writes to write
            + Good for parallelism between readers
            + Can starve writers
                - Readers can pass batton and never trigger okToWrite signal
            Code:
                semaphore_t numReadersLock;
                semaphore_t okToWrite;
                int         numReaders;

                void init{
                    numReadersLock.value = 1;
                    okToWrite.value = 1;
                    numReaders = 0;	
                }
                void writer (){
                    wait(okToWrite);

                    do writing (could pass
                in pointer to write function)

                    signal(okToWrite);
                }
                void reader (){
                    wait(numReadersLock);
                    numReaders++;
                    if (numReaders ==1){
                        wait(okToWrite); //not ok to write
                    }
                    signal(numReadersLock);

                    do reading (could pass in pointer to read function)

                    wait(numReadersLock);
                    numReaders--;
                    if (numReaders == 0) {
                        signal(okToWrite); //ok to write again
                    }	
                    signal (numReadersLock);
                }

        - Fair
            + Could do okToAccess instead of okToWrite and numReaders
                - Secure and fair, but no parallelism
            + Better solution is semephores numReadersLock, incomingQueue, nextInLine and int numReaders
                -  Writers
                    + First wait for incomingQueue, then wait for nextInLine
                    + once past nextInLine, signal incomingQueue to let someone else be nextInLine
                    + write
                    + signal new nextInLine to go
                - Readers 
                    + Wait for incomingQueue and numReadersLock
                    + once past numReadersLock, increment numReaders
                    + if numReaders = 1 then wait at nextInLine
                    + else
                    + signal numReadersLock to let someone in
                    + signal incomingQueue to let someone be next
                    + read
                    + wait for numReadersLock
                    + decrement numReaders
                    + if numReaders = 0, signal nextInLine
                    + signal numReadersLock
                Code:
                    semaphore_t numReadersLock, incomingQueue, nextInLine;
                    int         numReaders;

                    void init{
                        numReadersLock.value = 1;
                        incomingQueue.value = 1;
                        nextInLine.value = 1;
                        numReaders = 0;	
                    }
                    void writer (){
                        wait (incomingQueue);
                        wait (nextInLine);
                    
                        //Let someone else move on
                        //to wait on next
                        signal(incomingQueue);
                        
                        do writing
                        
                        signal (nextInLine);
                    }
                    void reader (){
                        wait(incomingQueue);
                                
                        wait(numReadersLock);
                        numReaders++;
                        if (numReaders == 1) {
                                wait (nextInLine);
                        } 	    
                        signal(numReadersLock);

                        //If next on incoming is 
                        //writer will block on next
                        //If reader will come in
                        signal(incomingQueue);

                        do reading

                        wait(numReadersLock);
                        numReaders--;
                        if (numReaders == 0){
                            signal (nextInLine);
                        }
                        signal(numReadersLock);
                    }

-- Slide Set 10 -- Deadlock ----------------------------------------------------------------------- 

    Dining Philosophers Problem:

          P C P         P is a philosipher
        P C   C P       C is a chopstick
        C       C       In this case N = 8
        P C   C P    
          P C P

        - have N philosophers and N chopsticks
            + Philospers think and eat
            + Philosophers must share the choptick to left and right of them 
              with the philosophers left and right of them
            + Represent with semephore chopstick[N]
            + philosopher i thinks until they are hungry
            + when they get hungry, they wait to grab chopstick[i]
            + once they do grab it, they wait for chopstick[i-1 % N] 
              (% N makes it so philosopher[n] grabs the one between them and philosopher[1])
            + they then eat until they put down the chopstick
            + once done with that, they signal that the two chopsticks are open for use
            + THIS CAN LEAD TO DEADLOCK!
                - If everyone decidedes to start eating at the same time they will first grab the chopstick to their right
                - Then they will wait to grab the chopstick to their left
                - Because everyone has 1 chopstick, everyone is waiting and no one can release the locks
        Code:
            semaphore_t chopstick[NUM_PHILOSOPHERS];

            void init(){
            for (i=0; i< NUM_PHILOSOPHERS; i++)
                chopstick[i].value = 1;
            }

            void philosophersLife(int i){
                while (1) {
                
                think(); 

                wait(chopstick[i])
                grab_chopstick(i);
                wait(chopstick[(i-1) % NUM_PHILOSOPHERS])
                grab_chopstick(i-1);

                eat();

                putdownChopsticks()
                signal(chopstick[i]);
                signal(chopstick[(i-1) % NUM_PHILOSOPHERS]);
                    
                } 
            }

        - Fixing dining philosophers
            + Make philosophers grab both chopsticks they need atomatically
                - Pass around a token (lock) saying whi can grab chopsticks
            + Force philosophers to put down chopsticks
                - Hard to make them go back
            + Better semephore
                - If i > i-1 % N then get chopstick[i] first
                - else get chopstick[i-1] first
                - this eliminates circular wait
            Code:
                void philosophersLife(int i){
                    while (1) {
                    think(); 
                    if ( i < ((i-1) % NUM_PHILOSOPHERS))}{
                            wait(chopstick[i]);
                            wait(chopstick[(i-1) % NUM_PHILOSOPHERS]);
                    } else {
                            wait(chopstick[(i-1) % NUM_PHILOSOPHERS]);
                            wait(chopstick[i]);
                    } 
                    
                    eat();

                    signal(chopstick[i]);
                    signal(chopstick[(i-1) %
                        NUM_PHILOSOPHERS]);

                    } 
                }

    Deadlock:
        - Deadlock exists in a set of processes and threads when all the processes/threads in the set
          are waiting for an event that can only be caused by another processes in the set
            + Dining philosophers is a perfect example because it models a situation where they all must wait for one 
              another to finish eating but in order to do that, they must wait for everyone else
        - Model deadlock with a deadlock allocation graph
            + each node in the graph represents a process/thread or a resource
            + an edge from node P to R indicates that process P requests resource R
            + an edge from node R to P indicates that process P holds resources R
            + If a graph has a cycle, deadlock MAY exist. If graph has no cycle, deadlock CANT exist
        - Deadlock can ONLY exist if only if the following four conditions are met
            + Mutually Exclusion: some resource must be held exclusively
            + Hold and Wait: some process must be holding one resource and waiting for another
                - Do not let processes hold a resource while waiting for another
                    + Make processes get all resources at once
                    + Windows WaitForMultipleObjects
                - Make processes get all needed resources at the beginning
                    + Disadvantage: May not need all resources the whole time
                    + Can release them early but must hold until used
                - Make processes release all resources before requesting more
                    + hard to program
            + No preemption: resources cannot be preempted (take them away)
                - Kill processes if they hold onto resources too long
                - Allow system to take back resources
                    + hard to program, system has to figure out how to take the resources without breaking proammers illusion
                    - Possibly use checkpoint and rollback
            + Circular Wait: there must be a set of processes (p1,p2,...,pn) 
              where p1 is waiting for p2, p2 is waiting for p3,... and pn is waiting for p1
                - Impose ordering on the resources
                    + hard to think of all of types of resources in system and order them
                    + how to prioritize one resource over another in order
                    
    Deadlock Avoidance:
        - Have processes decalre the maximum resources thay may ever request from the beginning
        - During exectuion, have system only grant a request if it can assure that all processes can run to completion without deadlock
        - Consider a set of processes (p1,p2,...,pn) which each declare the maximum resources they might ever request
        - When pi actually requests a resource, the system will only grant the request if the system could grant pi's maximum resource
          request with the resources currently availible plus the resources held by all the processes pj for j > i
        - May need p1 to complete then p2 all the way to pi but pi can complete

    Banker's Algorithm
        - Decide whether to grant a resource (a loan, give a philosipher a chopstick, give a process a lock)
        - Let there be P processes and R resources
        - Keep track of 
            + Number of units of each resource availible
            + Maximum number of units each resource that each process could request
            + Current allocation of each resource to each process
        - Simulate the completion of everyone
            + Very expensive so general purpose OS tend not to use it 
        Code: 
            unsigned available[R]; 
            unsigned allocation[P][R];
            unsigned maximum[P][R];

            startProcess(unsigned p){
                for (i=0; i< R; i++){
                    maximum[p][i] = max number of resource i that process p will need at one time; 
                }
            }

            BOOL request(unsigned p, unsigned r){
                if (allocation[p][r] + 1 > maximum[p][r]){
                    //p lied about its max
                    return FALSE;
                }

                if (available[p][r] == 0){
                    //cant possibly grant; none available
                    return FALSE;
                }

                if (canGrantSafely(p, r))
                    allocation[p][r]++;
                    available[r]--;
                    return TRUE;
                } else {
                    return FALSE;
                }
            }
            BOOL canGrantSafely(unsigned p, unsigned r){
                unsigned free[R]; 
                unsigned canFinish[P];
                
                for (j=0; j< R; j++) free[j] = available[j];
                for (i=0; i< P; i++) canFinish[i] = FALSE;

            lookAtAll: for (i=0; i< P; i++){
                allCanFinish = TRUE; 
                if (!canFinish[i])
                    allCanFinish = FALSE;
                    couldGetAllResources = TRUE;
                    for (j=0; j< R; j++){
                    if (maximum[i][j] - allocation[i][j] >  free[j]){
                        couldGetAllResources = FALSE;
                    }
                    }
                    if (couldGetAllResources){
                    canFinish[i] = TRUE;
                    for (i=0; i< R; i++) free[j] += allocation[i][j];
                    }
                }
            } //for all processes (lookAtAll)
                if (allCanFinish) {
                    return TRUE;
                }else {
                    goto lookAtAll;
                }
            }

    If you dont prevent deadlock:
        - Two choices:
            + Enable the system to detect deadlocks and if it does recover
            + Hope the deadlock never happens and rely on manual detection and recovery (i.e. kill process when it hangs)
    
    Deadlock Detection:
        - Can periodically look at the state of all the processes in the System
            + If process is holding anything, act like it exits
            + If the process is not requesting anything, act like it exits and return its resources to the system
              and see if that would allow any other processes to complete
                - If all processes complete that way great
                - If not, then left with knot or cycle in the grapgh
                - To solve it requires simulating completion of the processes like Banker's Algorithm
        - Unlike bankers, you can choose how often to run this, how many processes will be effected, when CPU usage drops below x%
    
    Recovering form deadlock:
        - How many?
            + Abort all deadlocked processes
            + Abort one process at a time until cycle is eliminated
        - Which ones?
            + lowest priority with canFinish = FALSE
            + One that has been running the least amount of time
            + Process that hasnt been killed before (any way to tell?)
        
    Prevention vs Avoidence vs Detection:
        - Spectrum of low resource utilization
            + Prevention gives up most chances to allocate resources
            + Detection always grants resource if they are available when requested
        - Also spectrum of runtime overhead
            + Prevention has very little overhead; programmer obeys rules and at runtime system does little
            + Avoidance uses bankers algorithm (keep max request for each process and then look before leap) 
            + Detection algorithm basically involves building the full resource allocation graph  
            + Avoidance and detection algorithms both expensive! O(R*P2)

-- Slide Set 11 -- Memory Management --------------------------------------------------------------

    Compilers
        - Compliers take human readable high level code and produce executable ones
        - OS executes the executables as processes
        - Compliers know what architecture the OS being compiled for needs
            + Architecture -> instruction set, registers, etc.
            + OS -> exectuable format like PE, ELF, or COFF (e.g. where to put main, global variables, eyc.)


    Address Space Map:
        Biggest ^  +----------------------------------------+ <--- Sometimes reserved for OS
        Virtual |  | Stack - Space for local variables etc. |
        Address |  |         For each nested procedure call |
                |  +----------------------------------------+ <--- Stack Pointer
                |  |       |                  ^             | <--- Biggest area
                |  |       V                  |             |      OS will typically stop you long before the stack and heap overlap but it can be changed  
                |  +----------------------------------------+
                |  | Heap - Space for dynamically allocated |
                |  | memory (e.g. with malloc)              |
                |  +----------------------------------------+
                |  | Staticly Decalre Variables             |
                |  | (Global Variables)                     |
                |  +----------------------------------------+
                |  | The Code                               | <--- PC
                |  | Text segment                           |
        0x0000  V  +----------------------------------------+ <--- Sometimes reserved for Error Catching

    In a 32 bit machine, each process gets 2^32 bits of space to use (4 Gb), 
    does each process really need all this space in memory at all times?
        - First, has it used it all?
            + lots of toom in the middle between heap and stack
        - Second, even it has if it has used a chunk of the adress space, is it using it actively right now?
            + Maybe lots of code that is rarely used (initilazation code used only at the beginning, error handling code, etc.)
            + Allocate space on the heap then de-allocate
            + Stack grows big once but then normally stays small
        - What to do with portions of address space never used?
            + Dont allocate until touched
        - What to do with rarely used portions of the address space?
            + Just because its rarely used doesnt mean its not necessary to store
                * Still a shame to waste that space, what to do
            + Why not send it to disk to get it out of our way?
                * disk not being used as non-volitile memory but more as a staging area
                * can help restore running processes after a crash
                * NEED TO REMEMBER WHERE IT WAS WRITTEN TO IN ORDER TO READ IT AGAIN!!!

    Logistics:
        - How to keep track of which regions are paged and where they are?
        - what will happen when a process tries to access a paged region on disk?   
        - How will DRAM and disk be shred with the File System?
        - Will we have a minimum size region that can be sent to disk?
            * A fixed block or page is useful to reduce fragmentation and for efficent disk access
        
    Virtual Memory:
        - Basic OS memory management abstraction/technique
        - processes use virtual addresses
            + every time a process fetches an instruction or loads a value into a register it refers to virtual memory address
        - OS (with help from the hardware) translates virtual addresses to physical addresses
            + must be fast as its always happeneing
            + OS guides the hardware on how to do this translation to avoid system call overhead
            + OS only takes control when hardware cant translate something
        - OS manages sending some portions of virtual address space to disk when necessary
            + Called paging
            + Sometimes translation will involve stalling to fetch page from disk
        - Provides protection and isolation among processes 
            + Processes can only use virtual memory addresses rather than the real ones
              Only the OS can interact with physical memory
              This prevents processes from being able to access other processes memory by accident
        - Provides illusion of more available system memory
            + OS tries to share its memory fairly among processes
                * "Your whole set of addresses is avalible and if you need them then the OS will bring them from disk to you"
            + Illusion breaks with heavy paging and out of memory errors
                * Out of memory error start before hitting maximum space used to prevent massive issues

            + Can one processes use so much memory taht other processes are forced to page heavily?
            + Can one processes use so much of the backing storage that other processes get out of memory errors?
        - Hardware support for virtual memory
            + Fast translations require hardware support, without it the OS would have to be invloved on every instruction execution
            + OS initilizes harware properly on context switch and then hardware supplies translation protection
        
    Simple Verion: Fixed Partitions:
        - OS can divide physical memory into fixed sized regions that are available to hold portions of the address spaces of processes
        - Each process gets a partition and so the number of partitions = the max number of runnable processses
        - Harware Support
            + Base registers
            + Physical address = Virtual address + base register
            + If Physical address > partitionsize then harware generates a faults
        - During context switch, OS will set the base register to the beginning of the new processes partition
        - Hardware could have another register that says the base virtual address in the partition
        - Translation/protection would then be
            + If virtual address generated by the process is between the base virtual address and base virtual address + length, 
              then access is ok and physical address is Virtual address - base virtual address Register + base register
            + Otherwise OS must write out the current contents of the partition and read the section of the address space being addressed now
            + OS must record location on disk where all non resident regions are written 
              (or record that no space has been allocated on disk or in memory if a region has never been accessed)
        - Problems with fixed size partitions
            + Must access contiguous portion of address space
                * Using both code and stack could mean a lot of paging
            + What is the best fixed size?
                * If everything a process needs is in the parition, it might need to get very big
                  (or at least need to cahnge how compilers work)
            + Paging such a big thing could take a long time
              (Especially if only using a small portion)
            + Also "best" size would vary by process
                * Some processes might not need all the "fixed" size while others need more than it
                * Internal fragmentation 
                  (Wasted space inside a block)

    Technique 2: Variable size partitions:
        - Very similar to fixed size partitions
        - Add a length register (no longer a fixed size) that hardware uses in translation/protection calculations 
          and that OS saves/ restores on context switch
        - No longer have a problem with internal fragmentation
        - May have external fragmentation
            + As processes are created and completed, free space in memory is likely to be divided into smaller pieces 
            + Could relocate processes to coalesce the free space
        - How does OS know how big to make wach process partition? Also how big does OS decide what is a fair amount to give each process?
        - Still have problem of only using only contigous regions

    Paging:
        - Could solve the external fragmentation problem, minimize the internal fragmentaion problem 
          and allow non-contigous regions of address space to be resident by breaking both physcial and virtual memory up into fixed size units
            + Smaller than a partition but big enough to make a read/write disk efficent often 4k/8k
            + often match FS - why?
        - How to find pages?
            + Any page of physical memory can hold any page of virtual memory from any process
                * How long are we going to keep track of this?
                * How are we going to do translation?
            + Need to map virtual memory pages to physical memory pages (or use disk locations or that no space is yet allocated)
            + Such maps are called page tables
                * One for each process (Virtual address x will map differently to physical pages for different processes)

    Page Table Entry:
        - Each entry on a page table maps virtual page numbers to (VPNs) to physcial page frame numbers (PFNs)
            + Virtual addresses have 2 parts: VPN and offset
            + Physical addresses have 2 parts: PFN and offset   
            + Offset stays the same is virtual and physical pages are the same size   
            + Are VPN and PFN the same size
        - Take a full virtual address and a full physical address and spit them into peices, a lower and upper portion
            + lower portion whould be the same size
            + Upper part returns a page number and lower part returns an offset

    
    +---- Translation --------------------------------------------------------------------+
    |                                                                                     |
    |    Virtual Address                                                                  |
    |    [[Virtual Page #][Offset]]                                                       |
    |         |                                                       [Page Frame 0]      |
    |   (translation)                                                 [Page Frame 1]      |
    |         |                                                                           |
    |         |   Page Table                                                              |
    |         |   [            ]                                                          |
    |         |   [            ]       Physcial address                                   |
    |         +-> [Page Frame #] ----> [[Page Frame #][Offset]] ----> [Needed Page Frame] |
    |             [            ]            |                             ^               |
    |             [            ]            |                             |               |
    |                                       + ----------------------------+               |
    |                                                                                     |
    |                                                                 [Page Frame n]      |
    |                                                                                     |
    +-------------------------------------------------------------------------------------+

    Example:
        - Assume a 32 bit address space and 4k page size
            + 32 bit address space implies virtual address have 32 bits and full address space is 4 Gb
            + 4k page length means offset is 12 bits (2^12 = 4k)
            + 32-12 = 20 so VPN is 20 bits
            + How many bits is PFN? Often 20 bits as well but wouldn't have to be 
              (enough to just cover virtual memory)
        - Suppose Virtual address 
          00000000000000011000|000000000111 or 0x18007
            + Offset is 0x7, VPN is 0x18
            + Suppose page table says VPN 0x18 translates to PFN 0x148 or 101001000
        - Physical address is 
          00000000000101001000|000000000111 or 0x148007

    +---- Example in Picture Form --------------------------------------------------------------------------+
    |                                                                                                       |
    |    Virtual Address - 0x18007                                                                          |
    |    [[00000000000000011000][000000000111]]                                                             |
    |         |                                                                             [Page Frame 0]  |
    |   (translation)                                                                       [Page Frame 1]  |
    |         |                                                                                             |
    |         |   Page Table                                                                                |
    |         |   [                    ]                                                                    |
    |         |   [                    ]       Physcial address - 0x148007                                  |
    |         +-> [00000000000101001000] ----> [[00000000000101001000][000000000111]] ----> [Page 0x148007] |
    |             [                    ]                    |                                   ^           |
    |             [                    ]                    |                                   |           |
    |                                                       + ----------------------------------+           |
    |                                                                                                       |
    |                                                                                       [Page Frame n]  |
    |                                                                                                       |
    +-------------------------------------------------------------------------------------------------------+
        
    Page Table Entries Revisited:
        - Entry can and does contain more than just a page frame number
                [[M][R][V][prot][           Page Frame Number           ]]
        - [M]odify bit: whether or not the page is dirty
            + Whether the copy in physical memory matches the version on disk
            + Birty pages cant be removed from physical memory without investing in a disk write to safely access later
        - [R]eferance bit: whether or not the page has been read/written recently
        - [V]alid bit: whether the page table entry contains a valid translation
        - [prot]ection bits: which operations are valid on this page
            * read
            * write
            * execute
        - Page Frame Number

    Processes' View of Paging:
        - Processes view memory as a contigous address space from 0 through n
            + OS may reserve some of the address sapace for its own use 
              (map OS into all processes address space is a certain range or declare some addressess invalid)
        - In reality, virtual pages are scattered across physical memory frames (and possibly paged out to disk)
            + Mapping is invisible to the program and beyond its control
        - Programs cannot referance memory outside its virtual address space because virtual address X will map 
          to different physical addresss for different processes

    Paging Advantages:
        - Avoid external fragmentation
            + Any phsyical page can be used for any virtual page
            + OS maintains list of free physical frames
        - Minimize internal fragmentaion (pages ar emuch smaller than partitions)
        - Easy to send pages to disk
            + Dont need to send a huge region at once
            + Use a valid bit to detect referance to paged out regions
        - Can have non-contigious regions of the address space resident in memory
    
    Paging Disadvantages:
        - Memory to hold page tables can be large
            + one PTE per virtual page
            + 32 bit address space with 4KB pages and 4 bytes/PTE = 4 MB of page table per process!!!
            + 25 processes = 100 MB of page tables!!!
            + Can we reduce this size?
                * Play same trick we did with address space - why have a PTE for virtual pages never touched?
                    ~ Add a level of indirection
                    ~ Two level page tables
        - Memory referance overhead
            + Memory referance means 1 memory accesss for the page table entry, doing the translation, 
              then 1 memory acess for the actual meory access
            + Caching translations
        - Still some internal fragmentaion
            + Process may not be use using memory in exact multiples of page size
            + Pages big enough to amortize disk latency
        
    Two Level Page Table: (Can have more in the real world i.e. n level page tables)
        - Add a level of indirection
        - Virtual addresses not have 3 parts: Master page number, secondary page number, and offset
        - Virtual Address
            [[        Virtual Page #         ][Offset]]
            [[Master Page #][Secondary Page #][Offset]]
        - Make master page table fit in one page
            + 4K page = 1024 4 byte patterns
            + So 1024 secondary page tables = 10 bits for master, still 12 for offset so 10 left for secondary
        - Invalid MPTE means whole chunk of address space not there
    
    +---- Two Level Translation --------------------------------------------------------------------------+
    |                                                                                                     |    
    |    Virtual Address                                                                                  |
    |    [[Master Page #][Secondary Page #][Offset]]---------------------------------------------+        |
    |         |              |                                                                   |        |
    |   (translation)        +---------------+                                                   |        |
    |         |      Master Page Table       |                                                   |        |
    |         |      [                    ]  |                                                   |        |
    |         |      [                    ]  |                                                   |        |
    |         +----> [Secondary Page Table] -+--> Secondary Page Table                           |        |
    |                [                    ]  |    [                  ]        Physical Address   V        |
    |                [                    ]  +--> [Page Frame #      ] -----> [[Page Frame #][Offset]]    |
    |                                             [                  ]                                    |
    |                                                                                                     |
    +-----------------------------------------------------------------------------------------------------+

    Page the Page Tables:
        - In addition to allowing MPTE's to say invalid could also say this secondary page table on disk
        - Master PTE for each process must stay in memory
            + Or maybe add another level of indirection?
            + Table mapping Master PTEs for each process to DRAM location of disk LBA
        - Each layer of indirection adds to access cost
        - Original page table scheme doubled the cost of memory access (one for page table, one for real memory location)
        - Two level triples the cost
        - Solve problem with caching

    Translation Lookaside Buffer (TLB):
        - Adds a hardware cache inside the CPU to store recent virtual page to page table Entries
            + Fast, one machine cycle for a hitting
        - OS doesnt even have to get involved when hit in the TLB
        - Usually a fully associative cache
        - Cache tags are virtual page numbers
            + Fast, all entries are seached/compared in parallel
            + Small, usually onl 16-48 entries (64-192 KB)
                * If process actively using more address space than that, it will get TLB misses
            + In harware, small often equals fast
        - Cache values are PTEs
        - TBL is managed by the memory management unit (MMU)
            + With PTE + offset, MMU can directly calculate the physical address
        - Manages to get >99% hits
        - Processes have very high degree of locality to their accesses patterns
            + When map a 4k page, likely access one memory location, that prefetched the rest
              and likely to access them next (so if 1 in 1042 4 byte accesses will be hits)

    What happens when a TLB Misses?
        - Hardware loaded TLBs
            + HArware knows where page tables are in memory (stored in register say)
            + Tables must be in HW defined format so that it can parse them
            + X86 works this way
        - Software loaded TLB
            + On TLB miss generate on OS fault and OS must find and load the correct PTE and then restart the access
            + OS can define PTE foramt as long as loads in foramt HW wants into the TLB
        - Either way, we have to choose one of current entries to kick out - which one?
            + TLB replacement policy usually as simple LRU

    Context Switch:
        - Contents of TLb reflect mapping from virtual to physical - that only applies to only one process
        - On context switch must flush the current TLB entries of things from last process
        - Could restore entries for new process (preload) or just set them all to invalid 
          and generate faults for first few accesses
        - This is a big reason context switches are expensive!
            + recall kernal level thread switch more expensive than user level switch

    Segmentaion:
        - Similar technique to paging except partition address space into variable sized segments reather than into fixed sized pages
            + Recall FS blocks vs extends
            + Variable size = more external fragmentaion
        - Segments usually correspond to logical units
            + code segment, heap segment, stack segment, etc.
        - Virtual address = [[Segment #][Offset]]
        - HW support? Often multiple base/limit register pairs, one per segment
            + stored in segment table
            + segment # used as index into the table

    Paging Segments?
        - Use segments to manage logical units and then divide them into fixed size pages
            + no external fragmentation
            + Segments are pageable so dont need whole segment in memory at time
        - X86 does this
    
    Linux on X86:
        - 1 kernal code segment and 1 kernal data segment
        - 1 user code segment and 1 user data segment
        - Belongs to process currently running
        - N task segments (stores registers on context switch)
        - All segments paged with 3 page tables

    Shared Memory:
        - Like building a wall between processes and then opening a door between some
            + Creates a virtual address space and allows both processes to read and write to it
            + A way to implement process communication
        - Exploit level of indirection between virtual address 
          and physical address to allow processes to communicate through shared memory
        - Map the same set of physical page frames into different virtual address space 
          (maybe at a different virtual addresses)
            + Each process has its own PTEs so it can give different processes different types of access
              (read/write/execute)
            + Execute access to same regions good for shared libraries

    Duplicates of large items:
        - Suppose two processes each want a private writable copy of the same data
            + Like fork splitting into 2 copies
                * copy-on-write
        - If it is small give them there own physical pages
        - They just want private writable copies so can't just use normal shared memory
        - If it is big painful to duplicate especially if they are each going to change a little bit

    Copy-on-Write:
        - Instead of copying, make a shared memory region but mark everyone's permissions as read only 
          (even though they really have permission to write)
        - Then if they try to write, HW will generate an access violation fault
            + OS invoked on faults and usually end processes
            + In this case, OS will make a copy of just the page written and then set the PTE to point to 
              the new private copy (with write privalages this time) and restart
            + Much like servicing a page fault where have to bring data in from disk
        - Copy-on-Write often used on fork to share a copy of the parents address space even though logically 
          parent and child each get their own provate writable copy (esp good because often wuickly overwritten)

    Memory Mapped Files:
        - Can access files thorugh the virtual memory system as well as through typical open/read/write FS interface
        - Map a file into a region of your address space
            + File start = address x
            + Then read file offset y = look at your data a memory location x+y
            + Write File offsetr y = set memory location x+y = new value
        - Doesn't read entire file when Mapped
            + Initally pages mapped to the file are invalid
            + When the memory mapped region. translated into FS read/write operations by the OS

-- Slide Set 12 -- Memory Management 2: Memory Boogaloo -------------------------------------------

    Overview
    Demand paging
        Start with no physical memory pages mapped and load them in on demand
    Page replacement Algorithms
        Belady  optimal but unrealizable
        FIFO  replace page loaded earliest
        LRU  replace page referenced earliest
        Working Set  keep set of pages in memory that induces minimal fault rate (need program specification)
        PFF  Grow/shrink page set as a function of fault rate
    Fairness  globally optimal replacement vs protecting processes from each other?


    Limited DRAM:
        - With paging we could probably "function" with just one resident memory page for each process
          (and its master page table)
        - But reading and writing memory pages to disk is expensive so we dont want to do it very often
        - So how much system DRAM do we really need for each process?
            + Do we give each process the same amount of memory?
            + Do they all need the same amount?
            + Do we have enough system DRAM to support all the processes we want to run? 
              (We know we can do better than 4 BG for each one but to avoid constant paging how many do we need?)
                * practical and theoretical answer

    Practical answer:
        - SIZE refers to virtual memory, RES refers to physical memory (In linux - page 3 of slides)
        - SIZE - RES is amount of non-volitile storage needed
            + using this to be bigger and cheaper than volitile memory not necessarily to be non-volitile
        - Can store to volitile storage with free space in memory if data is not being used
        - Observations about actual memory usage
            + Varies signifcantly per process
            + Are any processes paging too hevily?
                * could we tell from just the stats? how would we know?

    Theoretical answer:
        - How much memory do processes need?
        - "working set" of a processes is the set of virtual memory pages being actively used by the process
        - Define a working set over an interval
            + W Sp(w)= pages P accesses in the last w accesses
            + if w = total number of P accesses P makes then WSp(w) = every virtual meory page touched by P
        - Small working set = accesses of a process have high degree of locality
        - Changes in working Set
            + working set changes over the life of the processes
                * e.g. At first all the initalizization code is in the working set of a process but after 
                  some time it wont be any longer
            + Intuitively, you need to keep the working set of a processes in memory or the OS will constantly 
              be bringing pages on and off disk
            + Normally when we ask how much memory a given programs needs to run, 
              the answer is either its average or maximum working set (depending on how conservative you want to make your estimate)
            
    Demand paging
        - When a process first starts up
            + It has a brand new page table with all PTE valid bits set to false because no pages yet mapped to mysical memory
            + As processes fetches instructions and accesses data, there will be page faults for each page touched
            + Only pages that are needed or "demanded" by the processes will be brought in from disk
        - Eventually mat bring so many pages in that must choose some for eviction
            + Once evicted, if access, will once again demand page in from disk
        - When working set cahnges (like at the beginning of a process), you will get disk I/Object
        - But if most memory eccesses result in disk I/O the process will run *painfully* slow
        - Virtual memory may be invisible from a functional standpoint but certainly not from a performance one
            + There is a performance cliff and if you step off of it you are going to know
            + remember building systems with cliffs is not good

    Pre-paging:
        - Anticipating fault before it happens and prefetch the data
        - Overlap fetch with computation
        - Can be hard to predict and if predicted wrong evict something useful in excange
        - Programmers can give hints (vm_advise)

    Thrashing:
        - Spending all your time moving between pages and from disk and little time actually maknig progress
        - System is overcommited

    Avoid paging:
        - Given the cost of paging, we want to make it infrequent as possible
        - Fucntion of:
            + Degree of locality in the application (size of the working set over time)
            + Amount of physical memory
            + Page replacement processes
        - The OS can only control the replacement policy

    Goals of Replacement Policy:
        - Performance
            + Eveict a page that will never be accessed again if possible
            + If not possible, evict a page that wont be used for the longest time
            + How can we proedict this?
        - Fairness
            + When the OS divides up the available memory among processes, what is a fair way to do that?
                * Same amount to everyone? Well some processes may not need that for their working set 
                  and some may be paging to disk constantly with that amount of memory
                * Give each processes its working set?
            + As long as enough memory for each process to have its working set resident then everyone is happy
                * if not how do we resolve the conflict?
        - Algorithms
            + Remember all the different CPU sceduling algorithms the OS could use to choose the next job to run
            + Similarly, there are many different algorithms for picking which page to kick out when you have to 
              bring in a new page and there is no free DRAM left
            + Goal?
                * Reduce the overall system page fault rate?
                * Balance page fault rates among processes?
                * Minimize page faults for high priority jobs?

    Beleday's Algorithm:
        - Evict page that wont be used again for the longest time
            * Cant know this 100% for sure
        - Much like shortest jobs first
        - Has provably optimal lowest page fault rate
        - Difficult to predict which page won't be used for a while
            + Even if not practical can use it to compare other algorithms too

    First-In-First-Out (FIFO):
        - Evict the page that was inserted the longest time algorithms
            + When page put on tail of list
            + Evict head of list
        - Is always (usually) the case that the thing accessed the longest time ago will not be accessed for a long time
        - What about things accessed all the time?
        - FIFO suffers an interesting anomaly (Beleday's anomaly)
            + It is possible to increase the page fault rate by increasing the amount of memory

    Least-Recently Used (LRU):
        - Idea: the past is a good predictor of the future of the future
            + Page that we haven't used for the longest time likely not to be used again for longest time
            + Is past a good predictor
                * Generally yields
                * could be exactly the wrong thing! Consider streaming access
        - To do this requires keeping a history of past accesses
            + To be exact LRU would need to save a timestamp on each access (i.e. write the PTE on each access!)
            + Too expensive

    LRU Clock:
        - Also called Second Chance
        - Logically put on all physical page frames in a circle (clock)
        - Maintain a pointer to a current page
            + If ref bit off then evict
            + If ref bit on clear and move on (second chance)
        - Arm moves as wuickly as evictions are requested
        - If evictions rarely requested then arm moves slowly and pages have a long time to be referanced
        - If evictions frequently requested then arm moves fast and little time before the second cahnce is up

    Fairness?
        - All the replacment policies weve looked at so far just try to pick the page to evict regardless 
          of which which process the page belongs to
        - What if demand page in from one process causes the eviction of another processes page? Is that fair?
        - On the other hand is it fair for one processes to have 2 times their working set 
          while another has 1/2 their working set and is paging heavily?

    Fixed vs Variable Space:
        - Fixed Space algorithms
            + Give each process a limit of pages it can use
            + When it reaches its limit, it replaces LRU or FIFO or whatever from its pages
            + May be more natural to give process a say in the replacement policy used for its pages
        - Variable space algorithms
            + Processes set of pages grow and shrinks
            + One processes can ruin it for the rest but opportunity to make globally better decisions

    Use Working Set:
        - Could ask each process to inform the OS of the size of its working set
        - OS can only allow a process to start is it can allocate the complete working set
        - How easy for processes to report this?

    Page Fault Frequency:
        - PFF is a variable-space algorithm that tries to determine the wokring set size dynamically
        - Monitor pgae fault rtae for each process
        - If fault rate is above a given threshold, take away memory
        - Constant adjustment? Dampening factor so only changes occasionally

    Best Page Replacement:
        - It depends
        - Interestingly, if you have too much memory it doesnt matter
            + Anything will be ok (overprovisoning)
        - Also dosen't matter if you have too little memory
            + Thrashing and nothing you can do to stop it (overcommited)

-- Slide Set 13 -- Storage ------------------------------------------------------------------------

    Storage Hierarchy:
                 _________  Faster, Smaller, more expensive
                /Registers\            ^
               / L1 Cache  \            \  
              /  L2 Cache   \            \
             /   DRAM        \   Volitile \
        ----/-----------------\--------------
           /     NVRAM         \ Non-volitile
          /      DISK           \
         /       TAPE            \ (removable backup drives)
        +-------------------------+

    Example:
               ______      Faster, Smaller
              /Mind  \            ^
             / Pocket \            \  
            /  Backpack\            \
           /   Desk     \            \
          /    Bookshelf \ 
         /     The Attic  \
        +------------------+

    Secondary Storage:
        - "Secondary becuase unlike primary memory does not permit direct execution of instructions or data retrival via load/store instructions
        - Usually means hard disks
        - Tends to be larger cheaper and slower than primary memory
        - Persistant/non-volitile

    Tertiary Storage Devices:
        - Used primaraly as backup and archival storage
        - Low Cost is the defining characteristic
        - Often consists of removable media
            + CD-ROMS, tapes, etc.
        - As disks get cheaper and cheaper, duplicating data on multiple disks becomes 
          more and more attractive as a backup strategy

    Disk Basics:
        - Disk Drives contain metallic platters spinning around a central spindle                     
        - Read/Write head assembly is mounted on an arm that moves across the surface of the platters
        - Track: One ring around the surface of one of the platters
        - Sector: one peice of track (usually 512 bytes); more sectors in outer tracks
        - Cylinder: all tracks at the same distance from the center of the platters
          (i.e. all tracks readable without moving the disk arm)
    
    Disk Addressing:
        - Early Disks were addressed with a cylinder # surface # and sector #
        - Today disks hide information about their geometry
            + Disks export a logical array block
            + Disk itself maps from logical block addresses (LBA) to cylinder/surface/sector
            + Allows disk to remap bad sectors (when formatted disk reserves some sectors to use as replacements)
            + Allows disks to hide the non-uniformity of the storage
                * More Data on outer tracks
        - Disks also have internal caches so that not all requests go to the media
            + On reads take advanteage of multiple accesses to the same track
            + On Writes, say write is "done" when it is memory (not storage) inside the disk

    Disk Foramtting:
        - Low-level foramtting invloves dividing the magnetic media into sectors
            + Each sector actually consists of a header, data, and a trailer
            + Header and trailer cintain information like secotr number and error correcting codes (ECC)
            + ECC is additional redundant bits that can often correct for bit errors in the stored value
            + Help disk head settling (Figure out what platter you are)
        - OS also formats drives
            + 1st divides into partitions - each partition can be trated as a logically seperate drives
            + 2nd file system formatting of partitions (more on that later)

    Disk Interfaces:
        - Interface to the disk
            + Request specified with LBA and length
            + Request placed on bus, later repy placed on bus
        - Device driver hides these details
            + Provide abstraction of synchronous disk
        - OS use the disk to provide services
            + Virtual Memory
        - OS exports higher level abstractions
            + File systems
        - Some applicaitons use the device driver interface to build abstractions of their own
          (get their own partition)
            + Database Systems

    Disk Performance:
        - Divde the time for an access into stages
            + Seek time: time to move the disk arm to the corretc cylinder
                * How fast can mechanical arm move? Improves some with smaller disks but not much
            + Rotaional delay: time waiting for the correct sector to rotate under the read/write head
                * How fast can the spindle turn? RPMs go up but slowly
            + Transfer time: once head is over the right spot how long to transfer all the data
                * Larger for larger transfers
                * Rate determined by RPMs and by density of the bits on the disk (Density going up very quickly)
        - Getting good performance from drive (seeing impact of a "faster" drive means avoid seek and rotational delay)

    Avoid Seek and Rotational Delay:
        - To take advantage of higher transfer rates, OS must transfer larger and larger chunks of data at a time 
          and avoid seek and rotational delay
            + Size and placement of virtual memory pages
            + Size and placement of FS blocks
        - OS tries to avoid seek and rotational delay by placing things on disk together that will be accesed together
        - Can also avoid seek and rotational delay by queuing up multiple disk requests and servicing them in an order 
          that minimizes head movement (disk scheduling)
            + Like with CPU scheduling, there are many disk sceduling algorithms
    
    First Come First Serve (FCFS):
        - While fair, it dosent seek to solve seek and rotational delay

    Shortest seek time first (SSTF):
        - Selects the request with minimum seek time from the current head position
        - SSTF sceduling is a form of shortest job first (SJF) sceduling; may cause starvation of some requests

    SCAN (elevator sceduling):
        - The disk arm starts at one end of the disk nad moves toward the toher end, servicing requests until it gets to the otehr end of the disk,
          where the head movement is reversed and servicing continues.
    
    C-SCAN (Circular SCAN):
        - Provides a more uniform wait time than SCAN (with scan, those in middle wait less)
        - The head moves from one end of the disk to the other. servicing requests as it goes. When it reaches the other end, however,
          it immediatley returns to the begining of the disk, without servicing any requests on the return triples
        - Treats cylinders as circular list that wraps around from the last cylinder to the first one

    C-LOOK:
        - Version of C-SCAN
        - Arm only goes as far as the last request in each direction, the reverses direction immediatley, 
          without first going all the way to the end of the disk

    Selecting a Disk-Scheduling Algorithm:
        - SSTF is common and has a natural appeal
            + Starvation not observed to be a problem in practice
        - SCAN and C-SCAN preform better for systems that place a heavy load on the disk
        - Performance depends on the number and types of requests
        - Requests for a disk service can be infuenced by file-allocation method
        - Wither SSTF or C-LOOK is a reasonable choiice for the default algorithm

    Tracking Technology Trends:
        - Exact comaprison between technologies changes all the time
            + how much slower disk is than main memory
            + Variation even in disks and various memory technologies
        - Tracking these things takes a fair amount of work

    Non-Volitile Memory (NVRAM):
        - Memory that keeps it scontents when power removed
        - Used as secondary storage like disks
        - RAM = random access memory
            + DRAM = dynamic random access memory
            + NVRAM = non-volitile random access memory
        - Intermediate technology between DRAM and disk
            + Bigger and cheaper than DRAM
            + Smaller and more expensive than disk
        - Advantages
            + Solid state - no moving parts
                * AVoids seek and rotational latency of disk - so faster
                * uses less power than disk
                * less suceptable to shock from dropping (really good for portable devices)
        - Not jusy one NVRAM technology
            + Some of earliest was just battery backed DRAM
            + EPROM (erasable programmable read-only memory)
                * Erases with UV light
                * Erase whole thing to rewrite
                * Good for wrtie rarely, read mostly
            + EEPROM (electrically rasable programmable read-only memory)
                * Erases with higer voltage
                * Erase whole thing to rewrite
                * Good for wrtie rarely, read mostly

    Solid State Drives (SSD):
        - Disk drives made from NVRAM
        - Block based interface like disk
            + read/write logical block addresses
        - 2018: $220/1 Tb SSD and $90/1 Tb hard drive and ~5x faster
    
    Flash:
        - Erase a block at a time rather than all at once
            + NOR Flash resembles a NOR gate
                * Each cell has one end connect directly to ground, and the other end connected diretly to a bit line
                * Common for code starage and execution in cell phones, but NAND becoming popular there too
            + NAND Flash resembles a NAND gate
                * Several tranistors are connected in a series, and the bit line is pulled low only if all the word lines are pulled high
                * More cost effectively support smaller eras blocks comapred to NOR Flash
                * Common in cameras and SSDs
            - Flash Comaprison    | NOR  | NAND   |
              --------------------+------+--------+
              Cost Per Bit        | High | Low    | 
              File Storage Useage | Hard | Easy   |
              Code Execution      | Easy | Hard   |
              Capacity            | Low  | High   |
              Write Speed         | Low  | High   |
              Read Speed          | High | Lower  |
              Active Power        | High | Low    |
              Standby Power       | Low  | Higher |
              **Note: Higher and lower mean slightly higher or lower not a lot**

    HDD: Hard Disk Drives
    SSD: Solid State Drives
    SSHD: Solid-State Hard Drive
        - Regular HDD with an aditional NAND cache for storing more frequently used data and loading that data more quickly
    
    OS adapts to performance trends?
        - For the OS to make the right choices it needs to be aware of the trade-offs
            + Is the speed comparison between registers, DRAM and Disks like the diference between your mind, pocket and Bookshelf
              OR is it more like the diference between your pocket, the bookstore, and Pluto
            + How much computation/meta-data storage is reasonable to do to avoid a disk access?
            + Should we use DRAM as a file cacheor to store more memory page for processes?
        - "Right" answer changes with new generations of technology and OS source lives much longer than that?
        - Can OS measure performance and be coded to react to measurements?

-- Slide Set 14 -- RAID ---------------------------------------------------------------------------
    
    Redundant Array of Independant/Inexpensive Disks (RAID):
        - Basic ideas is to use a collection of disks working together, rather than just indepenedant
          disks owrking in isolation (Just a Bunch Of Disks (JBOD)), to acheive better reliability, performance, etc.
        - Type of storage virtualization, building an illusion of one logical storage device 
          out of many underlying physical storage devices
        - History
            + Originally proposed by David Patterson, Garth Gibson, and Randy Katz at UC Berkly
            + Proposed 5 original RAID levels or configurations (RAID 1-5)
                * Each level or configuration has different reliability, capacity, and performance properties based on 
                  how the physical disks are arranged/used
                * Some more for comparison - not really commercialy viable 
            + Sinse then, other RAID levels proposed

    RAID 1:
        - Mirroring
        - Writes go to both disks (slightly slower)
        - Reads go to either (faster)
        - If one disk fails, all data is still on the other
        - Storage capacity is cut in half
        - Block size can vary but is at least a sector (512 bytes)
        
        Disk 0      Disk 1
        +---------+ +---------+
        | Block 1 | | Block 1 |
        | Block 2 | | Block 2 |
        | Block 3 | | Block 3 |
        | Block 4 | | Block 4 |
        | Block 5 | | Block 5 |
        +---------+ +---------+

    RAID 2 & 3:
        - Other 2 original RAID levels
        - Both about bit interleaving of data
        - Show inspiration from memory redundancy
        - Not important commercialy

    RAID 4:
        - Blocks interleaved across multiple data disks in a stripe
        - Parity disk stores the XOR of the contents of the other blocks in the stripe
        - Parity: add data bits and parity is 1 if odd and 0 if even - sum of 1's always even
        - Parity can be used reconstruct contents of a drive that fails
        - Large writes - 1 extra disk write
            + Just have to rewrite the parity not read and update
        - Small writes - Read only, read parity, write new, write parity!
          Parity drive always invloved = bottleneck
            * Only one stripe can update at a time becuase the parity disk is already being written to
        - Recovery expensive

        Disk 0       Disk 1       Disk 2       Disk 3       Parity Disk
        +----------+ +----------+ +----------+ +----------+ +--------------+
        | Block 1  | | Block 2  | | Block 3  | | Block 4  | | Parity 1-4   |
        | Block 5  | | Block 6  | | Block 7  | | Block 8  | | Parity 5-8   |
        | Block 9  | | Block 10 | | Block 11 | | Block 12 | | Parity 9-12  | 
        | Block 13 | | Block 14 | | Block 15 | | Block 16 | | Parity 13-16 | 
        | Block 17 | | Block 18 | | Block 19 | | Block 20 | | Parity 17-20 | 
        +----------+ +----------+ +----------+ +----------+ +--------------+

    RAID 5:
        - Like RAID 4 but parity rotates
        - Releives bottleneck of the parity drive
        - Also all drives can participate in servicing small read requests (rather than n-1 for raid 4)
        - RAID 4 exists for comparison

        Disk 0           Disk 1           Disk 2           Disk 3           Disk 4
        +--------------+ +--------------+ +--------------+ +--------------+ +--------------+
        | Block 1      | | Block 2      | | Block 3      | | Block 4      | | Parity 1-4   |
        | Block 6      | | Block 7      | | Block 8      | | Parity 5-8   | | Block 5      |
        | Block 11     | | Block 12     | | Parity 9-12  | | Block 9      | | Block 10     |  
        | Block 16     | | Parity 13-16 | | Block 13     | | Block 14     | | Block 15     |  
        | Parity 17-20 | | Block 17     | | Block 18     | | Block 19     | | Block 20     |  
        +--------------+ +--------------+ +--------------+ +--------------+ +--------------+

    Ways to beat RAID 5 4x penalty:
        - 2 Reads and 2 Writes per small write
        - Do parity updates in the background in idle time
            + Of course if we lose data then any parity not yet updated is a problem
        - Have a large write cache so that you combine many small writes in the same stripe
            + If we lose power we lose recent data
            + Write cache from non-volitile RAM?
    
    RAID 0:
        - Striping 
        - Not one of the original 5 raid levels
        - Like raid 4 and 5 but no parity, ie no redundancy
        - If dont care about disk failure, this has best performance and no lost capacity

        Disk 0       Disk 1       Disk 2       Disk 3       Disk 4
        +----------+ +----------+ +----------+ +----------+ +----------+
        | Block 1  | | Block 2  | | Block 3  | | Block 4  | | Block 5  |
        | Block 6  | | Block 7  | | Block 8  | | Block 9  | | Block 10 |
        | Block 11 | | Block 12 | | Block 13 | | Block 14 | | Block 15 | 
        | Block 16 | | Block 17 | | Block 18 | | Block 19 | | Block 20 | 
        | Block 21 | | Block 22 | | Block 23 | | Block 24 | | Block 25 | 
        +----------+ +----------+ +----------+ +----------+ +----------+

    RAID 10 (1+0 or 1&0)
        - A stripe of mirrors
            + Highest level originzation is striping across logical drives (ie RAID 0)
            + Then each logical drive is made up of a mirrored pair (ie RAID 1)
        - Consider how availibility properties - how many drives could fail without data loss?
            + At least 1 and in some cases 2 (eg disk 0/1 and disk 2/4) 
        - Consider performance properties - large writes, large reads, small writes, small reads
            + Same as RAID 1, write to both disks in logical disk and read either disk in the logical disk

        Logical Disk 0            Logical Disk 1
        +-----------------------+ +-------------------------+
        |Disk 0      Disk 1     | |Disk 2       Disk 3      |
        |+---------+ +---------+| |+----------+ +----------+|
        || Block 1 | | Block 1 || || Block 2  | | Block 2  ||
        || Block 3 | | Block 3 || || Block 4  | | Block 4  ||
        || Block 5 | | Block 5 || || Block 6  | | Block 6  ||
        || Block 7 | | Block 7 || || Block 8  | | Block 9  ||
        || Block 9 | | Block 9 || || Block 10 | | Block 10 ||
        |+---------+ +---------+| |+----------+ +----------+|
        +-----------------------+ +-------------------------+ 

    RAID 01 (0+1 or 0&1)
        - Mirror of stripes
            + Highest level orginzation is mirroring across logical drives (ie raid 1)
            + Then each logical drive is made up of a stripe (ie RAID 0)

        Logical Disk 0            Logical Disk 1
        +------------------------+ +------------------------+
        |Disk 0      Disk 1      | |Disk 2       Disk 3     |
        |+---------+ +----------+| |+---------+ +----------+|
        || Block 1 | | Block 2  || || Block 1 | | Block 2  ||
        || Block 3 | | Block 4  || || Block 3 | | Block 4  ||
        || Block 5 | | Block 6  || || Block 5 | | Block 6  ||
        || Block 7 | | Block 8  || || Block 7 | | Block 9  ||
        || Block 9 | | Block 10 || || Block 9 | | Block 10 ||
        |+---------+ +----------+| |+---------+ +----------+|
        +------------------------+ +------------------------+ 

    RAID naming conventions:
        - Last number is the highest level originization
        - closest number is the lowest level organization
        - RAID 51
            + Highest level orgnization is mirroring across logical drives (ie raid 1)
            + Then each logical drive is made up of a distributed parity drives (ie RAID 5)
        - RAID 15
            + Highest level orgnization is distributed parity drives (ie RAID 5)
            + Then each distributed parity drive is mirrored across logical drives (ie raid 1)
        - RAID 100
            + Highest level orginization is striping across a series of >2 logical drives (ie RAID 0)
            + Middle level orginization is striping across logical drives (ie RAID 0)
            + Then each logical drive is made up of a mirrored pair (ie RAID 1)
    
    RAID 10 vs RAID 5:
        - Sample comparison
        - Raid 10 has less usable capacity than raid 5 (1/2 to redundancy vs 1/n)
        - RAID 10 avoids the 4x small write penalty
        - RAID 10 cant use n-1 spindles for large writes though
        - Both can use all drives for reading in pararllel
        - RAID 10 can handle multiple drive failure in some cases

    RAID 6:
        - Always tolerate 2 disk failures
        - Extra disk and in each stripe two used to store parity information
            + Actually 2 different types of parity calculation
        - instead of 2 accesses for a small write have 6
        - Expensive but given how instensive recovery is in RAID 5, chances of second failure while recovering
          is relitively high especially considering the chance of corelated failures
            + Dont want a second drive failure at just the wrong time
    
    Correlated faiures:
        - Drives exposed to the same workload, temperature etc.
        - If buy drives from the same manufacturer at the same time, can also be corrlated failures there
        - Can be good to buy a bunch of same size drives from multiple vendors
            * Careful, one slow or small drive can drag others down

    Hot Spares:
        - Literally an empty replacment drive hooked up in case of a catastrophic failure in an active drive in the RAID
        - Common to leave unused disks of the right side in the RAID
        - If disk fails, sont want to run in degredated state, want to complete recovery quickly
        - Dont want silent failure though because need to order new drive etc
        - some systems "phone home" on their own and order parts

    Proprietary RAID levels:
        - All sorts of non standard RAIDXX levels
            + RAID DP. RAID S, RAID 5EE, RAID 50EE, RAID Z, vRAID, RAID 1E

    Hardware vs Software Raid:
        - RAID can be inplemented in either software or hardware
        - Hardware RAID exports a signle disk image to the OS and hides details to a HW appliance
        - Software RAID implements RAID functionality out of JBOD

    Configuration Advice:
        - Matching stripe size to workload
        - Large files, large streaming access => large stripe size
        - Small files, small random access => small stripe size

    Still need backup:
        - Controller Error
        - Human Error
        - Site Disaster

-- Slide Set 15 -- File Sytem Basics --------------------------------------------------------------


     Files:
        - A File is a collection of data within a system maintained by properties like
            + owner, size, name, last read/wrtie time, etc.
        - Files often have "types" which allow users and applicaitons to recognize their intended use
        - Some file types are understood by the file system
            + Mount point, symbolic link, directory
        - Some file types are understood by applications and users
            + .txt, .jpg, .html, .doc 
            + Could the system understand these types and customize its handling?

    Directories:
        - Provide a way for users to organize their files *and* a convient way forusers to identify and share data
        - Logically directories store information like file name, size, modification time etc (Not always kept in the directory though)
        - Most file systems support hierarical directories
            + /user/local/bin/ or C:/WININT
            + People like to organize information hierarically
        - Recall an OS often records a current working directory for each process
            + Can therefore refer to files by absolute and reletive names
        - Directories are special files
            + Directories are files containing information to be interpreted by the file system itself
                * List of files and other directories contained in this directory
                * Some attributes of each child including where to find it
            + How should the list of children be organized?
                * Flat File
                * B-tree?
            + Many Systems have no particular order, but this is extremely bad for large directories
        - Multiple parent directories
            + One natural question is "can a file be in more than one directory?"
            + Soft Links
                * Special File interpreted by the file system (like directories in that sense)
                * Tell FS to look at different pathname for this file
                * If file is deleted or moved, soft link will point to wrong place
            + Hard Links
                * Along with other file info maintain referance count
                * Delete File = decrement referance count
                * Only reclaim stroage when referance count goes to 0

    Path Name Translation:
        - To find file "/foo/bar/baz"
            + Find the special root directory file (how does FS know where it is)
            + In the special root directory file, look for entry foo and that entry will tell you where foo is
            + Read special directory file foo and look for where entry bar is
            + Find special directory file bar and look for entry baz to tell you where baz is
            + Finally read baz
        - FS can cache common/recent prefixes for efficiency
            + Translation is expensive
    
    File Buffer Cache:
        - Cache Data read
            + Exploit temporal locality of access by caching pathname translation information
            + Exploit temporal locality of access by leaving recently accessed chinks of a file in memory
              in hopes that they will be accessed again
            + Exploit spacial locality of access by bringing in large chunks of a file at once
        - Data written is also cached   
            + For correctness should be write through to disk
            + Normally is write-behind
                * FS periodically walks the buffer cache and "flushes" things older than 30 seconds to disk
                * unreliable
        - Usually LRU replacement
        - Typically cache is system wide (shared by all processes)
            + Shared libraries and executables and other commonly accessed files likely to be in memory already
        - Completes with virtual memory for physical memory
            + Processes have less memory available to them to store code and data (address space)
            + Some systems have integrated VM/FS caches

    Protection System:
        - Most FS implement a protection scheme to control
            + Who can access a file
            + How they can access it
        - Any protection system dictates whether a given action performed 
          by a given subject on a given object should be allowed. In this case
            + Objects = files
            + Principles = users
            + Actions = operators
        
    File Layout:
        - Option 1: all blocks in a file must be allocated contigiously
            + Only need to list start of a file and length in directory
            + Causes fragmentation of free space
            + Also causes copying as files grow
        - Option 2: Allow files to be broken into peices
            + Fixed size pieces (blocks) or variable sized pieces (extends)?
            + If we are going to allow filest obe broken up into multiple pieces how will we keep track of them?
    
    Blocks Vs Extends:
        - If fixed sized blocks then store starting location of each one
        - If variable sized extent need to store starting location and length
            + But maybe have fewer extends
        - Blocks = less external fragmentation
        - Extends = less internal fragmentation

    Finding all parts of a file
        - Option 2A: List all bokcs in the directory
            + Directories will get pretty big and also must change the directory every time a file grows or shrinks
        - Option 2B: Linked structures
            + Directory points to first piece (block or extend), first one points to second one
            + File can expand and contract without copying
            + Good for sequential access, terrible for other kinds
        - Option 2C: Indexed structure
            + Directory points to index block which contains pointers to data blocks
            + Good for random access as well as sequential access

    UNIX Inodes
        - Inode = indexed node
            + Files Broken into fixed size blocks 
            + Inodes contain pointers to all the file blocks
            + Directory points to location of inodes
        - Each inode contains 15 block pointers
            + First 12 point directly to data blocks
            + Then single, doubly and triply indirect blocks
        - Inodes often contain information like last modification time, size etc.
          that could logically be associated with a directory
        - NOTE: indirect blocks sometimes numbered as file blocks -1, -2, etc.

        Inode
        +------------------------------+
        | Size                         |
        | Last mod                     |
        | Owner                        |
        | permissions                  |
        |+------------------------------+
        |LBA of file block 1           | ----> [File Block]
        +------------------------------+
        |LBA of file block 2           | ----> [File Block]
        +------------------------------+
        |        ...                   | ----> [File Block]
        +------------------------------+
        |LBA of file block 11          | ----> [File Block]
        +------------------------------+
        |LBA of file block 12          | ----> [File Block]
        +------------------------------+
        |LBA of singly indirect block  | ----> [LBAs of file blocks] ----> [File Blocks]
        +------------------------------+
        |LBA of doubly indirect block  | ----> [LBAs of singly indirect file blocks] ----> [LBAs of file blocks] ----> [File Blocks]
        +------------------------------+
        |LBA of triply indirect block  | ----> [LBA of doubly indirect blocks] ----> [LBAs of singly indirect file blocks] ----> [LBAs of file blocks] ----> [File Blocks]
        +------------------------------+

    Max File Size?
        - Assume
            + 4K data pages and inodes
            + Lbas are typically 4 bytes
        - First 48k directly reachable from inode
        - One singly indirect block reaches 1024 more blocks = 4M
        - one doubly indirect block points to 1024 more singly indirect blocks which each poin to 4M of data = 4GB
        - One triply indirect block points to 1024 more doubly indirect blocks which each poin to 4GB of data = 4TB
        - Max file or directory size = 4TB + 4GB + 4MB + 48KB

    Path Name Traversal Revisited:
        - Directories are just special files so they have inodes of their own
        - To find /foo/bar/baz (assuming nothing is cached)
        - Look in super block and find location of I-node for /
        - Read inode for /, find location of first datablock of /
        - Repeat with all 13 blocks of / until find every entry for foo, if read until block 13, then must read singly indirect block first...
        - When find entry for foo gives adress of Inode for foo, read inode for foo...
        - Reapeat down the path

    Keeping track of free space:
        - Linked list of free space
            + Just put freed blocks on the end and pull blocks from front to allocate
            + Hard to manage spatial locality (why important?)
            + If middle gets corruted how do you repair?
        - Bit Map
            + Divide up all space into blocks
            + Bit per block (0 = free, 1 = allocated)
            + Easy to find groups of nearby blocks
            + Useful for disk recovery
            + How big? If I had 40 BG disk, then have 10 M of 4K blocks, if each needs 1 bit then 10M/8 = 1.2 MB for the bit map

-- Slide Set 16 -- FFS and LFS --------------------------------------------------------------------

    Fast File System (FFS)
        - Fast? Wwll faster tahn the original UNIX files system in 1970
            + original system had poor disk bandwidth
            + too many seeks
        - BSD Unix folks resdesigned in mid 1980s
            + Imporved disk utilization by breaking files into larger pieces
            + Made ffs aware of disk structure (cylinder groups) and tried to keep related things together
            + Other semi random improvemtns like support for long file names etc.
        - Break disk into cylinder groups and then into fixed size pieces called blocks (commonly 4KB)
        - Each cylinder group has a certain number of blocks
            + Cylinder group's free list maps which blocks free and which taken
            + Cylinder groups also store a copy of the superblock which contains special bootstrapping information
            location of the root directory (replicated)
            + Cylinder groups also contain a fixed number of inodes
            + Rest of blocks used to store file/directory data
        - Inodes in FFS
            + Fixed number of inodes at FS format time
                * When we create the file, pick an inode, will never move (so directory entry need not be updated)
                * Can run out of inodes and not be able to create file even though there is free space
        - Creating a new file 
            + Divide the disk into cylinder groups
                * Try to put all blocks of file into same cylinder group
                * Inodes in each cylinder group so inodes near thier files
                * Try to put files in teh same directory into the same cylinder group
                * Big things forced into new cylinder group
            + Is this fundamentally a new approach?
                * No, space within a cylinder group gets treated just like the whole disk was
                * Space in a cylinder group gets fragmented etc
                * Bascily sort files into bins so reduce the frequent long seeks
        - Cynlinder groups
            + To keep things together we must know when to keep things apart
                * Put large files into different cylinder groups
            + FFS reserves 10% of teh disk as free space
                * To be able to sort things into cylinder groups, must have free space in each cylinder group
                * 10 % free space avoids worst allocation choice as it approaches full
        - Other FFS Imporvements
            + small or large blocks?
                * orig unix FS had small blocks (1KB)
                * 1/4 less efficent BW utilization
            + Larger blocks have problems too
                * for files < 4K, results in internal fragmentaion
                * FFS uses 4K blocks but allows fragments within a block
                * Last, > 4k of a file can be in fragments
            + Exactly 4k
                * FFS allows FS to be paremeterized to the disk and CPU characteristics
                * another cool example: when laying out logically sequential blocks, skip a few blocks in between each to 
                allow for the CPU interrupt processesing so dont justmiss the blocks and force a whole rotation
        -Update in place
            + Both the original UNIX FS and FFS were update in place
            + When block X ofa file is written then forever more, 
            reads or writes to block x go to that location until file deleted or truncated
            + As things get fragmented need "defragmenter" to reorganize things
        - Another problem with update in place: poor crash recovery performance
            + Some operations take multiple disk requests so are imposible to do atomically
                * atomically - either everything happens at once or nothing does
                * Ex. Write a new file (update directory, remove space from free list, write inode and data blocks, etc.)
            + If system crashes (lose power or software failure), there may be file operations in progress
            + When a system comes back up, may need to find a fix to these half done operations
            + Where are they?
                * Could be anywhere?
                * How can we restore consistency to the file system
        - Fixed order --------- TODO: for user file system ---------
            + Solution: specificly order in which FS ops are done
            + Example to add a file
                * Update free list structures to show data block taken
                * Write the data block
                * Update free list structures to show an inode take
                * Write the inode
                * Add entry to the directory
            + If a crash occurs, on reboot scan disk looking for half done operations
                * Inodes that are marked taken but are not refered to by any directory
                * Data blocks that are marked taken but are not referred to by any inode
            + Weve found a half done operations, now what?
                * If data is not pointed to by any inode then release them
                * If inode not pointed to by any directory link into lost and found
            + fsck and silimar FS recovery programs do these kinds of checks
                * Problems can be anywhere with update in place so must scan the whole FS
            + Problems?
                * Recovery takes a long time
                * Even wrose(?) normal operation takes a long time because specific order = many small synchonous writes = slow

    Write-Ahead Logging (Journaling):
        - How can we solve problem of recovery in update in place systems?
        - Borrow a technique from dataabses!
            + Loging or Journaling
        - Before preform a file system operation like create new file or move a file, make a note in the log
        - If crash, can simply examine the log to find interrupted operations
            + Dont need to examine the while disk
        - Checkpoints
            + periodically write a checkpoint to a well known location
            + Checkpoint establishes a consistent point in the file system
            + Checkpoint also contains pointer to tail of the log (changes since checkpoint written)
            + On recovery start at checkpoint and then "roll forward" through the log
            + Checkpoint points to location system will use for first log write after checkpoint, then each log write after checkpoint, 
              then each log write has pointer to next location to be used
                * Eventually go to noxt location and find it empty or invalid
            + When writing a checkpoint, can discard earlier portions of the log
        - Problems
            + Do writes twice
                * Once to log and once to "real" data (Still organized like FFS)
            + Surprisingly can be mor efficent than update-in-place
                * Batched to log and then replayed to "real" in relaxed order (elevator sceduling on the disk)
        - Recovery of the file system (not your data)
            + Write-ahead logging or journaling techniques could be used to protect FS and user data
            + Normally just used to protect the FS
            + I look like a consistent FS but your data may be inconsistent
                * Even if some of the last files you were modifying are inconsistent still better than FS corrupted
            + Still why do we need a "real" data layout why couldn't the log be the FS? The user data would get the same benifits

    Log-Structured File Systems
        - Treat the disk as an infinite append only log 
            + Data blocks, inodes directories everything written to log
        - Batch wrtites in large units called segments (~1MB)
        - Garbage collection process called cleaner recalims holes in the log to regenerate large expanses of free space for log writes
            + As files get deleted or updated, this leaves a hole in log segments. Cleaners go thorugh and try to clear out whole segments and fill the 
              data that was in a partially empty segment into the tail of the log so as to keep segments free and limit fragmentation
        - Finding data
            + Inodes used to find datablocks
            + Finding inodes?
                * directories specify location of a file's inode
            + in an FFS, inodes are preallocated in each cylinder group and given file's inode never moves
            + In an LFS, inodes written to the log and so they move
        - Chain reaction
            + LFS is not update in place when a file block written its location changes
                * File location changes => entry in inode (and possibly also indirect blocks) changes
                  => Inode (and indirect blocks) must be rewritten
            + Parent directory contains location of inode  must directory be rewritten too? 
                * If so then all directories to root must be rewritten?
            + No, introduce another level of indirection
                * Directory says inode *number* (rather than location)
                * inode map to map inode number to current location 
        - Inode map
            + Inode map maps inode numbers to inode location
                * map kept in a special file called the ifile 
            + When a files inode is written, its parent directory does not change, only the ifile does
            + Caching inode map (ifile) in memory is pretty important for good performance
                * how big is this? Approx 2*4 bytes (inode number and disk LBA) = 8 bytes for every file/directory in the file system
                * Can grow dynamically unlike FFS
        - Checkpoint
            + Like in write ahead logging, write periodic Checkpoints   
                * kind of like FFS superblocks
            + Checkpoint region has a fixed location
                * Actually two fixed locations and alternate between them in case it dies in the middle of writing and is left corrupt
                * Checksums to verify consistent; timestamps say which was most recent
            + Whats in checkpoint?
                * Location of inode for ifile and inode number of the root directory
                * Location of next segment will write log to
                * Basic FS parameters like segment size, block size, etc.
        - Pros 
            + Leverage disk BW with large sequential writes
            + Near perfect write performance
            + Read perfomance? Good if read the same way as you write and many reads absorbed by cache
            + Cleaning can often be done in idle time
            + Fast, efficent crash recovery
            + User data egts benifits of a log
        - Cons
            + Reads may not follow write patterns (they may not follow directory structure either though)
            + Aditional metadata handling (inodes, indirect blocks and ifile rewritten frequently)
            + Cleaning overhead can be high - especially in case of random updates to a full disk with little idle time
        - Cleaning costs 
            + We are going to focus on talking about the problem of high cleaning costs
            + Often cleaning is not a problem 
                * If there is plenty of idle time (many workloads have this), cleaning costs hidden
                * Also if locality to writes, then easier to clean
                * If disk not very full then, segments clean themselves (overwrite everything in old segments before run out of free spaces for new writes)
            + So when is cleaning a problem?
                * Cleaning expensive when random writes to full disk with no idle time
                * Random writes, full disk (little free space), no idle time = Sky-rocketting cleaning costs
                * For every 4 blocks written, also read 4 segments and write 3 segments!
        - Copy cleaning vs hole plugging
            + Alternate cleaning method?
                * Hole-plugging = Take one segment break extract the live data and use it to plug holes in other segments
                * This will work well for full disk, random updates, little idle time!!
            + Hole-plugging avoids problems with copy cleaning but transfers many small blocks which uses the disk less efficiently
                * Could we get the best of both worlds?
                * First we have to talk about how to quantify the tradeoffs
        - Write cost
            + How do we quantify the benifits of large I/Os vs the penalty of copying data
            + Original LFS paper evaluated the efficiency of cleaning algorithms according to the following metric
                * (DataWrittenNewData + DataReadCleaning + DataWrittenCleaning)/ DataWrittenNewData
                * Quantifies cleaning overhead in terms of the amount of data transferred while cleaning
                * What about the impact of large vs small transfers?
            + Cost of small transfers
                * Quantify overhead due to using the disk inefficiently
                    ~ TransferTimeActual/TransferTimeIdeal
                    ~ Where TransferTimeActual includes seek and rotational delay and transfer time and TransferTimeIdeal only includes transfer time
                * By factoring in the cost of small transfers, we see the cost of holeplugging
            + Overall write cost
                * Ratio of actual to ideal costs where
                    ~ Actual includes cost of garbage collection and includes seek/rotational latency for each transfer
                    ~ Ideal includes only cost of original writes to an infinite append only log  no seek/rotational delay and no garbage collection
                * Now we have a metric that lets us compare hole-plugging to copy-cleaning that the systen can use to choose which one to do
        - Adaptive Cleaning
            + When starting to run out of segments, do garbage collection
            + look in special file called the segmap that tells you how full each segment is
                * when rewriting a block in a segment, write in segmap file that segment is one block less full
            + Estimate cost to do copy cleaning and cost to do hole-plugging 
                * Compute overall write cost by seeing how full segments are
            + Choose the most cost effective method this time
                * Can be different next time
        - Other factors?
            + How does this layout work for reads?
                * Good if in the same way you write
                * Well until start reorganizing during cleaning
                  (hole plugging is worse than copy cleaning here)
                * Special kind if hole-plugging that writes back on top where it used to be?
            + Accounting for aditional metadata handling in cache?
                * Modifying the write cost metric to account for "churn" in the metadata?
                * Model FFS in this same way

    Improving FFS
        - Extent like performance (McVoy)
        - FFS-realloc (McKusick)
        - FFS-frag and FFS-nochange(Smith)
        - Colocating FFS (Ganger)
        - Soft Updates (Ganger)

    Other FS
        - Update-in-place
            + FAT
            + ext2 (extent based rather than fixed size blocks)
        - Write-ahead Logging (journaling)
            + NTFS
            + ReiserFS (B+ tree indices, optimizations for small files)
            + SGIs XFS (extent based and B+ trees)
            + Ext3 (journaling version of ext2)
            + Veritas VxFS
            + BeOSs BeFS
        - No Update?
            + CD-ROM FS no update and often contiguous allocations (why does that make sense?) 

    Network/Distributed FS
        - NFS
        - AFS and Coda
            + Transarcs (now IBMs) commercial AFS
            + Intermezzo (Linux Coda like system)
        - Netwares NCP
        - SMB

    Multiple FS?
        - With all these choices, do we really need to choose just one FS for our FS
        - If we want to allow multiple FS in the same OS, what would we have to do?
            + Merge them into one directory hierarchy for the user
            + Make them obey a common interface for the rest of the OS
    
    Mount Points
        - Another kind of special file interpreted by the file system is a mount point
        - Contains information about how to access the root of a separate FS tree 
          (device information if local, server information if remote, type of FS, etc.)

    Common Interfaces
        - Struct vnode
            + One vnode structure for every opened (in-use) file
            + Contains
                * Array of pointers to procedures to implement basic operations on files
                * Pointer to parent FS
                * Pointer to FS that is mounted on top of this file (if any)
                * Reference count so know when to release the vnode
            + Vnode Operations
                * Open, close, create, remove, read, write
                * Mkdir, rmdir, readdir
                * You dont know what that FSs directory format will be
                * Symlink, Link, readlink (soft/hard links)
                * Getattr, setattr, access (get/set/check attributes like permissions)
                * Fsync
                * Seek
                * Map, getpage, putpage (memory map a file)
                * Ioctl (misc I/O control ops)
                * Rename
        - Struct vfs
            + One vfs structure in the OS for each mounted fs
            + Contains
                * Array of pointers to procedures that implement basic operations on file systems
                * FS type
                * Native block size
                * Pointer to vnode this FS is mounted on
            + vfs Operations
                * Mount: procedure called to mount a FS of this type on a specified vnode
                * Unmount: procedure to release this FS
                * Root: return root vnode of this Fs
                * Statvfs: return research usage status of the FS 
                * Sync: flush all dirty memory buffers to persistent storage managed by this FS
                * Vget: turn a fileId into a a pointer to vnode for a specific file
                * Mountroot: mount this FS as the root FS on this host
                * Swapvp: return vnode of file in this FS to which the OS can swap

    Do we need an FS Interface?
        - Giving things file names seems arbitary
        - FS hierarchy vs directory search
        - People like to find information both ways
            + I know exactly what I want dont bother looking for me I will get it myself
            + Give me everything matching these characteristics

-- Slide Set 17 -- Networks -----------------------------------------------------------------------

    Network Code in OS:
        - Many Protocols from differenl layers of the networking stack
        - TCP and UDP
            + Programer decides which to use
            + UDP better for streaming videos
                * If you miss a frame, drop it insead of showing old frame
            + TCP is more reliable so its better for communication
                * more overhead
        - Client side IP
            + IPV4 and IPV6
            + IPV4 more smaller but there are more IPV6 addresses
        - ICMP
        - Device drivers for network interface cards

    Protocol Stack:
        +-------------------------------------------------------------------+
        |                                                                   |
        |   Host A                                                 Host B   |
        |   HTTP                                                     HTTP   |
        |    |                                                         ^    |
        |    V                                                         |    |
        |   TCP                                                       TCP   |
        |    |                                                         ^    |
        |    V               Router R           Router W               |    |
        |   IP                IP --+             +-> IP                IP   |
        |    |                ^    |             |    |                ^    |
        |    V                |    V             |    V                |    |
        |   Ethernet -> Ethernet  Link ------> Link  Ethernet -> Ethernet   |
        |                                                                   |
        +-------------------------------------------------------------------+

    Sockets:
        - A door between application level processes and end-end-trabsoirt protocol (UDP or TCP)
        - Socket Api
            + Introduced in BSD4.1 UNIX, 1981
            + Sockets are used, created, and released by applications
            + OS implements the socket itself
            + Two types of transport service via socket
                * Unrealiable Datagram (UDP)
                * Reliable, byte stream-oriented (TCP)
        - Sockets vs File Systems
            + Similar
                * Programming with files vs sockets
                * With files you dont say the type of file - goal is to kide those details from user logically
            + Different 
                * File systems typically cant read each other's disk formats 
                * Sockets can always communicate across networks by design

    Multiplexing/Demultiplexing:
        - Multiplexing: Gathering data from multiple application processes on the same host and sending out the same network interface
        - Demultiplexing: Stream of incoming data into one machine seperated into smaller streams desined for individual processes
        - Demultiplexing based on IP addresses and port number for boththe sender and receiver

    UDP: RFC 768
        - "No frills", "bare bones" Internet transport protocol (IP Layer)
        - "best effort" service, UDP segments may be:
            + Lost
            + Delivered out of order          
        - Connectionless:
            + No handshaking between UPD sender and receiver
            + Each UDP segment handled independantly of others  

        <----------- 32 Bits ----------->
        +---------------+---------------+
        | Source Port # | Destin Port # |
        +---------------+---------------+
        |     Length    |   Checksum    |
        +---------------+---------------+
        |       Application Data        |
        |              ...              |

        - Entire Header is 8 bytes
        - Ports 
            + 16 bits each
                * 16 bits so, 2^16 or 64K possible ports - not enough for whole Internet
                * Is Ok for just a single host
            + Used for multiplexing and demultiplexing
        - Length
            + 16 bits
            + used to find end of data
        - Checksum
            + 16 bits
            + Used to detect errors
            + Sender    
                * Treat segment contents as sequence of 16-bit integers 
                    ~ add 0 pad to get 16 bit chunks if necessary (10001101 -> 0000000010001101)
                * One's complement of the sum of all the 16-bit words in the segment
                * Sender puts checksum value into UDP checksum field
            + Receiver
                * Compute checksum of recived segment
                * check if computed checksum matches the checksum feild:
                    ~ No - Error detected
                        > Will always be an error if it fails
                    ~ Yes - Maybe no error detected
                        > Errors may be in other places like data or the checksum itself

    Port Implementation:
        - Message Queue
            + Append incoming message to the end
            + Much like mailbox file
            + Choose which message queue based on <src ip, dest ip + port>
        - If queue is full, message will be discarded
            + Best effort delivery
            + Network doesnt guarentee not to drop, so the OS needn't guarentee that either
            + If process context switched out for too long or if not actively reading from the socket
        - When application reads from socket, OS removes some bytes from the head of the queue
        - If queue is empty, application blocks waiting

    TCP: RFCs: 793, 1122, 1323, 2018, 2581
        - Point-to-point
            + One sender and one receiver (at a time)
        - Reliable, in-order byte stream
            + No "message boundries" like with UDP datagrams
        - Pipelined
            + TCP congestion and flow control, set window size
        - Send & Receive buffer
        - Full duplex data
            + Bi-directional data flow in same connection
            + MSS: Maximum segment size
        - Connextion oriented
            + Handshaking (Exchange of control msgs) init sender, receiver state before data Exchange
        - Flow Control and Congestion Control
            + Sender will not overwhelm the receiver

        <----------- 32 Bits ----------->
        +---------------+---------------+
        | Source Port # | Destin Port # |
        +---------------+---------------+
        |        Sequence Number        |
        +-------------------------------+
        |    Acknowledgement Number     |
        +-+-+-+-+-+-+-+-+---------------+
        |h|-|U|A|P|R|S|F|rec window size|
        +-+-+-+-+-+-+-+-+---------------+
        |    Checksum   |ptr urgent data|
        +---------------+---------------+
        | Options (variable length)     |
        +-------------------------------+
        |       Application Data        |
        |       (Variable Length)       |
        |              ...              |

        - U: Urgent data (generally not used (0))
        - A: Acknowledgement number valid
        - P: Push data now (generally not used (0))
        - R, S, F: RST, SYN, FIN: Connection establishment (setup, teardown commands)
        - rec window size: Number of bytes receiver is willing to accept
        - Sequence Number and Acknowledgement Number count by bytes of data (not segments)

        - Same as UDP
            + Ports
            + Checksum
        - Reliable delivery
            + Sequence number 
                * 32 bit
                * Number all the bytes uniquely
                * Sequence Number Feild indicates number of first byte in teh packet
            + Acknowledgement Field
                * 32 bits
                * Receiver sends feedback on what it has
                * The acknowledgement field contains the next sequence number that it is expecting thus implicitly acknowledging all previous segments
                * Cumulative acks not individual acks or negative acks
            + Receiver window size
                * Only let the receiver send so much so don't overwhelm the receiver
        
        - Example
            + Cant talk to person on the other side any other way
            + Number the pages  so sender can put back together
            + Let receiver send you a fax back saying what pages they have and what they still need (include your fax number on the document!) 
            + What if the receiver sends their responses with a flaky fax machine too?
            + What if it is a really big document? No point in overwhelming the receiver. Receiver might like to be able to tell you send first 10 pages then 10 more
            + How does receiver know when they have it all? Special last page? Cover sheet that said how many to expect?

    Internet Network Layer (IP Layer):
        +-------------------------------------------------------------------------------+
        | Tranport Layer                                                                |
        +-------------------------------------------------------------------------------+
        | Network Layer                                                                 |
        |   +-----------------+    +-------------+      +---------------------------+   |
        |   |Routing Protocols| -> |Routing Table| -+-> | IP Protocol               |   |
        |   +-----------------+    +-------------+  |   +---------------------------+   |
        |   | Path Selection  |                     |   | Addressing Conventions    |   |
        |   | RIP, OSPF, BGP  |                     |   | Datagram Format           |   |
        |   +-----------------+                     |   |Packet handling conventions|   |
        |                                           |   +---------------------------+   |
        |                                           |                                   |
        |                                           |   +------------------+            |
        |                                           +-> | ICMP Protocol    |            |
        |                                               +------------------+            |
        |                                               | Error Reporting  |            |
        |                                               |Router "Signaling"|            |
        |                                               +------------------+            |
        +-------------------------------------------------------------------------------+
        | Link Layer                                                                    |
        +-------------------------------------------------------------------------------+       
        | Physical Layer                                                                |
        +-------------------------------------------------------------------------------+

    Internet Protocol (IP):
        - The internet is a network of heterogenous networks
            + Using differernt technology 
                * ex. different maximum packet windows
            + Belonging to different administrative authorities
                * ex. willing to accept packets from different addresses
        - Goal of IP is to connect all these netowrks so can send end to end without any knowledge of the intermediate networks
        - Routers: Machines to forward packets between heterogenous netowrks

        <----------- 32 Bits ----------->
        +---+------+----+---------------+
        |Ver|Head L|Type| length        |
        +---+------+----+----+----------+
        | 16 bit Id     |flgs| Frag off |
        +---+-----------+----+----------+
        |ttl|upper layer|Internet Chksum|
        +---+-----------+---------------+
        | 32 bit source ip address      |
        +-------------------------------+
        | 32 bit destination ip address |
        +-------------------------------+
        | Options (if any)              |
        +-------------------------------+
        |       Application Data        |
        |       (Variable Length,       |
        |Typically a UDP or TCP segment)|
        |              ...              |

        - Ver: IP protocol version
            + 4 bit
            + 4 for IPV4 (0100)
            + 6 for IPV6 (0110)
        - Head L: header length
            + 4 bit
            + NUmber of 32 bit words (2^4-1 32 bits = 60 bytes)
            + Includes length of options (40 bytes max)
        - Type: "type" of data
        - Length: Total datagram length
            + 16 bits
            + length in bytes
            + Max total length 2^16-1 = 65535 bytes
            + Max data = 65535 - Header length
            + Other layers may not be able to handle this length
                * Link layer technologies have different limits
                * Can fragment if necessary
                * Total length will change when fragmented
        - ttl: Time to live or remaining number of hops between Routers
        - Upper layer: which upper level protocol to deliver to
            + 8 bits
            + 1 = ICMP
            + 2 = IGMP
            + 6 = TCP
            + 17 = UDP
            + 135 - 254 = Unassigned
            + Managed by IANA 
        - Checksum
        - Options - EG timestamp, record route taken, specify list of routers to visit
        - 16-bit ID: 16 Bit identifier  -+
        - flgs: Flags                    +--For Fragmentation/reassembly
        - Frag off: fragment offset     -+

    IP Fragmentation:
        - Network links have MTU (max transfer size)
            + Different n=link types have different MTU
            + ex. Ethernet maximum is 1500 bytes FDDI maximum is 4500 bytes
        - Large IP datagram divided ("fragmented") within net
            + Reassembled at final destination
            + May be fragmented multiple times
            + One fragment dropped => entire datagram dropped
        - IP Header Fields
            + Identification 
                * 16 bits
                * Unique ID for datagram
                * Original spec said to transport layer would set
                * Usually set to value of variable in IP layer that is incrimented by one for each datagram sent from that host
                * Wraps every 65535 datagrams
            + Flags
                * 3 bits
                * 1 flag to say whether there are more fragments following this one
                * 1 bit used to say "do not fragment" (drop and send error back if it cant be sent)
                    ~ ICMP Handles error
            + Fragment Offset
                * Give offset of data in this fragment into original datagram

        One large datagram becomes multiple small ones
        +-----+---------------+--------+--------------+------------+-----+
        | ... | Length = 4000 | ID = x | Fragflag = 0 | offset = 0 | ... |
        +-----+---------------+--------+--------------+------------+-----+
        |
        |
        +-> +-----+---------------+--------+--------------+------------+-----+
        |   | ... | Length = 1500 | ID = x | Fragflag = 1 | offset = 0 | ... |
        |   +-----+---------------+--------+--------------+------------+-----+
        |
        +-> +-----+---------------+--------+--------------+---------------+-----+
        |   | ... | Length = 1500 | ID = x | Fragflag = 1 | offset = 1480 | ... |
        |   +-----+---------------+--------+--------------+---------------+-----+
        |
        +-> +-----+---------------+--------+--------------+---------------+-----+
            | ... | Length = 1040 | ID = x | Fragflag = 0 | offset = 2960 | ... |
            +-----+---------------+--------+--------------+---------------+-----+

    Internet Control Message Protocol (ICMP):
        - Used by hosts, routers, and gateways to communicate network level info like errors or querying conditions
        - network-layer "above" IP
            + ICMP messages carried in IP datagrams
        - ICMP Message
            + Type
            + code
            + checksum 
            + plus typically first 8 bytes of IP datagram causing error
        -Error Condidtions
            + Host, network, port, or protocol unreachable
            + Cant fragment despite needing to
            + Source Quench
                * Intended for congestion control but not used
        - Query/Response
            + ICMP used to request IP related info
            + Query/Response
                * Adress mask request/reply
                * Timestamp request/reply
            + Replys echo identifier from query to be matched
                * Ping sends echo request to host and waits for response

-- Slide Set 18 -- Virtual Machines ---------------------------------------------------------------
    
    Different Models:
        - Virtual Machines Monitor or Hypervisor is a layer between the traditional OS and the harware
        - Allows the same physical machine to run multiple "guest" OS images
        - Each Virtual Machine gets illusion that it has its own harware and that it is running in isolation
    
    New Unit of Parallelism:
        - Thread = stack, instruction pointer
        - Process = resources allocated + threads inside
        - Virtual Machine = OS + processes inside

    Advantages of a Virtual Machine:
        - Save space, cost, and power by reducing number of physical machines
        - Co-locate multiple idle machines - each can have different OS, differnt configuration
        - Stronger isolation than processes running on the same OS (unit of sandboxing)
        - Guest OS needn't support such a wide variety of hardware
        - Enables virtual machines to be moved, even with diverse harware
        - Way to exploit multicore systems 

    Disadvantages of a Virtual Machine:
        - Another layer of overhead and translation
        - Isolation bwteen guest OSes not always reliable
        - New Layer to attck - especially as the footprint and complexity of interfaces grows

    Common uses:
        - Datacenters
        - Remote Hosting
        - Testing
        - Honeypots
        - Even Personal Computing
    
    Virtual Machine Monitor vs OS:
        - VMM provide a convienient way to use the same physical computer hardware for many different tasks at once
        - OSes do the same
        - VMMs take isolation further - increases the things you can't even name or observe directly
        - Oses could and some try to do the same thing
    
    Emulation:
        +--------------+--------------+--------------+
        | Applications | Applications | Applications |
        +--------------+--------------+--------------+      ...
        |  Unmodified  |  Unmodified  |  Unmodified  |
        |Guest OS for A|Guest OS for A|Guest OS for B|
        +--------------+--------------+--------------+--------------+
        |Harware for Virtual Machine A|Harware for Virtual Machine B|
        +-----------------------------+-----------------------------+
        |             Physical Harware Architecture P               |
        +-----------------------------------------------------------+

        - Simulate the complete harware interface for a different harware architecture than is actually present on the machine
        - Can be used to write/test an OS for a machine that doesnt exist yet or run software for one platform on another

    Full virtualization:
        +--------------+--------------+             
        | Applications | Applications |              +--------------+
        +--------------+--------------+              |  Hypervisor  |
        |  Unmodified  |  Unmodified  |      ...     |  Management  |
        |Guest OS for P|Guest OS for P|              |  Interface   |
        +--------------+--------------+--------------+--------------+
        |           Hypervisor (Virtual Machine Monitor)            |
        +-----------------------------+-----------------------------+
        |             Physical Harware Architecture P               |
        +-----------------------------------------------------------+

        - Similar to emulation but presenting a prefect replica of the underlying arhitecture only
        - Also called native virtualization
        - Can run many things on the actual underlying hardware but need to set up control/trust boundries between VMs
        - Done cheaper than emulation

    - Some architecture is easier to virtualize than others
    - Some designed for Virtualization like IBM Mainframes    
    - x86 notoriously difficult to virtualize
        + Major producers of x86 hardware introducing hardware extensions specifically to improve ability to virtualize 
        + HVM = hardware virtual machine or hardware-assisted virtualization
        + Intel VT-X or AMD-V 

    Paravirtualization:
        +--------------+--------------+             
        | Applications | Applications |              +--------------+
        +--------------+--------------+              |  Hypervisor  |
        |  modified    |  modified    |      ...     |  Management  |
        |Guest OS for 1|Guest OS for 2|              |  Interface   |
        +--------------+--------------+--------------+--------------+
        |           Hypervisor (Virtual Machine Monitor)            |
        +-----------------------------+-----------------------------+
        |             Physical Harware Architecture P               |
        +-----------------------------------------------------------+

        - Modifies the UNderlying HW interfaces but only where necessary
        - Avoid instructions that are expensive to virtualize or introduce some new "instructions"
        - Require the Guest OS to be modified to run on this new HW interface (Strategic cahnges not wholesale cahnges)
        - Harder for proprietary guest OS 

    Operating System Level Virtualization:
        +------------------+------------------+                  +------------------+
        | Private Server 1 | Private Server 2 |       ...        | Private Server n |
        +------------------+------------------+------------------+------------------+
        |                     Single Operating System Image                         |
        +---------------------------------------------------------------------------+
        |                    Physical Harware Architecture P                        |
        +---------------------------------------------------------------------------+

        - Looks like virtualization but really one OS providing isolation "containers"
        - Just get one version of OS - not for wanting to run multiple OS on same machine
        - Sometimes called paenevirtualization
        - Less duplication of resources, but generally worse isolation

    Virtual Machine Type Examples:
        - Emulation Parallels, PearPC, Bochs, QEMU
        - Full Virtualization: KVM, VMware, Parallels, z/VM, Xen (HVM)
        - Paravirtualization: Xen (non HVM)
        - OS-Level: Solaris containers, Linux VServers

    - Some can be mixes
    - Full virtualization of a cpu but Paravirtualization of devices is common
    - Xen runs Paravirtualized on non-HVM machines but fully virtualized on HVM machines
    - Allow guest OS not to focus on device drivers for every device- can be real advantage for new OSs
    - Another distinction - Layer 1 vs Layer 2
        + Layer 1 Hypervisor: Bare metal Hypervisor (directly on top of harware)
        + Layer 2 Hypervisor: Application running on top of an OS
    - Other types of virtualization that are not capable or running as full guest OS
    - Library Virtualization that emulates an OS (eg wine library for Linux)
    - Application level virtualization - runnin applications in a virtual execution environment (eg Java Virtual Machine)

    Management Features:
        - Snapshot
        - Rollback
        - Migration of VMs to new hardware- temporary or permenantly
        - Deployment of software in VMs
        - Introspection into VMs
        - Replay of input
        - VMs become digital object - not intricately tied to physical object

-- Slide Set 19 -- Parallel and Distributed Computing ---------------------------------------------
    
    Distributed Systems:
        - Computation and resources distributed over a set of network connected computers (across some physical distance/space)
            + May not be focused on solving the same problem just shared resources
    
    Parallel Computing:
        - Multiple pieces of computation working togther to solve a problem
            + May or may not be distributed, could be multiple cores or CPU/GPU etc.

    Why Parallel and Distributed Computing?
        - Resource Sharing
        - Computational speedup
        - Reliability

    Different Models:
        - Client Server
        - N-Tier 
            + eg. Front end web server, back end database, middle tier services (application server)
        - Tightly Coupled (Clustered)
            + Usually need not generally be aware of multiplicity of machines; single system illusion
        - Peer-to-Peer
            + No master nodes or special machine
            + Responsibilities shared among all participants
        - Job Queue
            + Workers take tasks from shared task list

    Different Communication Methods:
        - TCP/IP networking
        - Message passing (inter-process communication)
        - Shared datastore/datsbase
        - Harware Sharing - eg. shared memory

    OS Support for resource sharing:
        - Distributed OS can manage diverse resources of nodes in system
        - Make resources visible on all nodes
            + Like virtual memory, can provide functional illusion but rarly hide the performance cost

    OS Support for Process Management:
        - OS could manage all pieces of a parallel job as one unit
        - Allow all pieces to be created, managed, destoryed at a single command line
        - Fork (process, machine)

    OS Suport for Process Migration:
        - Execute an entire process, or parts of it at different stripes
            + Load balancing: distribute processes across network to even the workload
            + Harware preferance: process execution may require specialized processors
            + Software preferance: required software may be availible at only a particular Site
            + Data access: run process remotely, rather than transfer all data locally

    OS support for sceduling:
        - Distributed OS could scedule processes to run near the needed resources
            + If need to access data in a large databse may be easier to ship code there 
              and results back than t orequest data be shipped to code
        - Programmer could specify where pieces should run and or OS could decide
            + Processes Migration? Load Balancing?
        - Try to scedule piece together so can communicate effectively

    OS Support for Pararllel Jobs:
        - Group Communication?
            + OS could provide facilities for pieces of a single job to communicate easily
            + Location independant adressing?
            + Shared Memory?
            + Distributed FIle System?
        - Syncronization
            + Support for mutally exclusive acess to data acress multiple machines
            + Cant rely on HW Atomic operations any more
            + Deadlock management?
            + Data Coherency?
    
    Computational Speedup:
        - Some tasks are too large for even the fastest single computer
            + Real time weather/climate modeling, human genome project,
              fluid turbulence modeling, ocean circulation modeling, internet search, etc.
        - What to do?
            + Leave the problem unsolved?
            + Engineer a bigger/faster comuter?
            + Harness resouces of many smaller machines in a distributed system?

    Breaking up the problems
        - To harness computaional speedup must first break up the big problem int oamny smaller problems
        - More art then science
            + Sometimes break up by functiuon
                * Pipeline
                * Job queue
            + Sometimes break up by data
                * Each node responsible for portion of dataset
    
    Decomposition examples:
        - Decrypting message or SETI@home (easy)
            + Easy to parallelize, give each node a set of keys to try
            + Job queue - when tried all you keys go back for more?
        - Modeling ocean circulation (hard)
            + Give each node a portion of the ocean to model (N square foot reqion?)
            + Model flows within region locally
            + Communicate with nodes managing neighboring regions to model flows into other regions
        - Barnes hut
            + Calculating the effect of bodies in space on each other
            + Could devide up space into nxn chunks 
                * Some regions have more bodies
            + Instead divide up regions by number of bodies
                * Within a region bodies have lots of effect on each other (close together)
                * Abstract other regions as single body to minimize communication

    Linear Speedup:
        - Often the goal
            + Allocate N nodes to the job, it goes N times as fast
        - Once you've broken up the problem into N pieces, can you expect it to go N times as fast?

    Sub-Linear Speedup:
        - Are the pieces equal?
        - Is there a piece of the owrk that cannot be broken up (inherently sequential?)
        - Synchronization and communication overhead between pieces

    Super-Linear Speedup:
        - Sometimes actually do better than linear speedup!
        - Expecially id you divide up a big data set so that the piece needed at each node fits into main memory on that machine
        - Savings from avoiding disk I/O can outweigh the communication/synchronization costs
        - When you split up a problem, tention between duplicating processes at all nodes for reliability and simplicity and allowing nodes to specialize

    Reliability
        - Distributed system offeres potential for ind=creased reliability
            + If one part of the system fails, the rest could take over
            + redundancy, fail over
        - BUT Often reality is that distributed systesm offer less reliability
            + "A distributed system is one in which some machine I've never heard of fails and I cant do any work"
            + Hard to get rid of all dependencies
            + No clean failure mode
                * Nodes don't just fail they can continue in a broken state
                * Partition network = many many nodes fail at once!
                * Networks goes down and up and down again
            + More machines you involve = mor elikelyhood that some failure somehwere is the common case

    Tradeoff 
        - Clearly a between increased reliability and computational speedup
        - Additional resources can be dedicated to new tasks (speedup) or redundancy (reliability)
        - Similar to our discussion of raid
    
    Robustness:
        - Detect and recover from site failure. function transfer, reintegrate failed site
            + Failure detection
                * Detecting hardware failure is difficult
                * To detect a link failure, a handshaking protocol can be used
                * Assume site A and Site B have established a link. At fixed intervals, each site will exhage an I-am-up message indicating that they are up and running
                * If Site A does not receive a message within the fixed interval, it assumes either (a) the other site is not up or (b) the message was lost
                * Site A can now send an Are-you-up? message to site b
                * If site A does not recive a reply, it can repeat the message or try an alternate route to site B
                * If Site A ultimately does not recive a reply from site B, it concludes some type of failure occured
                * Types of failures:
                    ~ Site B is down
                    ~ The direct link between A and B is down
                    ~ The alternate link between A and B is down
                    ~ The message has been lost
                * However, site A cannot determine why the failure occured
                * B may be assuming A is down at the same time
                * Can either assume it can make decisions alone?
            + Reconfiguration
                * When Site A determines a failure has occured, it must reconfigure the system
                    ~ If the link between A to B has failed, this must be broadcast to every site in the system
                    ~ If a site has failued, every other site must also be notified that the services offereb by the failed site are no longer available 
                * When the link or the site becomes availible again, this information must again be broadcast to all other sites

    Falicies of Distributed Computing:
        1. The network is reliable.
        2. Latency is zero.
        3. Bandwidth is infinite.
        4. The network is secure.
        5. Topology doesn't change.
        6. There is one administrator.
        7. Transport cost is zero.
        8. The network is homogeneous.

    Byzantine generals problem:
        - Deals with reaching agreement in the face of both faulty communucations and untrustworthy paremeters
        - Easier if all peers are FAIL-STOP meaning if they fail, they all stop together
        - Problem
            + Divisions of an army each commanded by a general surrounding an enemy comapred
            + Generals must reach agreement on whether to attack (a certain number must all attack or defeat is certain)
            + Divisions are geographically seperated such that they must communicate via messagengers
            + Messenger may be caught and never reach the other side (lost messages)
            + Generals may be traitors (faulty/compromised processes)
        
        - Problem 1: Lost Messagers/Messages
            + How to deal with the fact that messages may be lost?
                * Clear connection to the networking protocols material
            + Detect Failures with a timeout scheme
                * When sending a message, specify a time interval to wait for an acknowledgement
                * When reciving a message send an acknowledgement
                * Ack can be lost too!
                * Ff recives the acknowledgement message within the specified time interval can conclude that the message was received.
                * If a time out occurs, retransmit message and wait for acknowledgement
                * Continue until an ack is received or give up after some time?
                * The last word
                    ~ Suppose the receiver needs to know that the sender has received the ack in order to decide how to procede
                    ~ Actually in the presence of a failure this is imposible
                    ~ In a distributed environment, processes Pi and Pj can not agree completely on their respective states
                    ~ Always levels of uncertainty about last message
        - Traitors (faulty processes)?
            + What could a traitor do?
                * Refuse to send messages
                * Delay sending messages
                * Send incorrect messages
                * Send different messages to different generals
        
    Example:
        - Consider a system of N processes, of which no more than m are faulty. 
        - Devise an algorithm that allows each non-faulty Pi to construct a vector Xi (Ai1, Ai2, ..., Ain)      
            + Each process Pi has some private value of Vi 
            + If Pj is nonfaulty then Aij = Vj
            + If Pi and Pj are both nonfaulty then, Xi=Xj
        - Solutions
            + Assume reliable communication
            + bound Maximum number of traitors to M
            + Correct algorithm can only be devised if n >= 3m+1
            + Worst case delay for reaching agreement is proportiante to m+1 message-passing delays
        - m = 1, n = 4
            + 4 >= 3m+1 requires m+1 = 2
            + Each process sends its private value t othe other 3 processes
            + Each process sends the inforamtion it has to all other processes
            + If faulty processes refuses to send messages, a nonfaulty process can chose an arbitrary value and pretend that value was sent by that process
            + After the two rounds are completed, a nonfaulty process Pi can construct its vector Xi = (Ai1, Ai2, Ai3, Ai4) as follows
                * Aij = Vj
                * for j != i if at least 2 of the three values reported for process Pj agree, then the majority value is used to set the value of Aij,
                  otherwise a default value (nil) is used
        - If m = 1, n = 4
            + No conclusion could be made

-- Slide Set 20 -- Protection ---------------------------------------------------------------------

    Protection:
        - Protecting proceses/users from each other is one of the core OS responsibilities
        - Control Access of processes or users to resources of the computer system (HW and SW)
            + Ensure resources are only operated on only those processes that have gained proper authorization
            + Enforcing resource limits

    Policy vs Mechanism:
        - Mechanism says "What types of access are possible" and "defines the means for identifying authorized vs unauthorized access"
        - Policy says "Which processes/users should have which kinds of access"
        - When buildingg system best to make mechanism match the problem domain rather than a particular desired policy
            + More flexible if you seperate mechanism from policy
        - Example: If your mechanism does not distinguish between read and exectue rights then imposible to hand out one without the other
          If mechanism does distinguish then policy may never chose to hand out one wihtout the other but it could

    Principles:
        - Genearlly the more restrictive the system the more protection
        - "Need to know" priniple says only grant those rights absolutely necessary to accomplish a task
            + Start out granting none and see where it breaks, add the smallest new privalages as possible
            + Ex. if a process only needs to read/write one specific file then dont give it access to all the user's files
            + Ex. Dont give full root privalages just because need to open port < 1024

    Cross-Cutting issue:
        - CPU Sceduling
            + Protection by timer interupts and OS scheduling policy
        - Process management
            + Protection by access control and enforcment of resource limits (most OS?)
        - Virtual Memory
            + Protection by inability to name other processes memory space
        - File System
            + User defined access controls per file/directory
        - NOTE: Synchronization more volentary protection by observing rules within a set of processes/threads that share data (monitors maybe protections?)
    
    How to do protection?
        - From that breif survey of OS topics it is clear that protectioncan be accomplished in many ways
            + Protection can be based on the design of the system which makes access imposible
              (cant even name things you shouldn't access)
                * Eg. virtual memory
            + Protection can be controlled by an OS wide policy
              (OS controls resource allocation)
                * Eg. timer interupts
            + Protection can be controlled by user definable access controls
                * Eg. user can set FS access controls
        - Implies ability to deny authorized access! Ability to enforce the policy!

    Where doe the OS get the ability to enforce?
        - From the hardware
        - Harware typically has at least two models
            * user and system (sometimes more)
        - On boot, OS registers handle for lal HW and SW interupts and begins in system mode
        - From then on transitions from system/privileged mode to user mode and bac kare controlled by the OS

    Recall Kernal/User mode:
        - Hardware needs to be able to distinguish the OS from user apps
            + Controls ability to execute privileged instructions etc
            + One way: mode value in a protected register
            + When user applications execute, the mode value is set to one thing
            + When the OS kernel executes, the mode value set to something else
            + If code running in user mode, an attempt to execute protected instructions will generate an exception
            + Switching the mode value must of course be protected

    Is Kernal/User Distinction enough?
        - Not if you want to distinguish between users  
            + how to do?
        - Is user the best thingto base domain on?
            + Do you want all processes you run to have your full privileges?
            + Do you ever need special privalges but not all of root access?

    Logging in:
        - Recall: in last stages of boot process, OS creates a process calles init
        - Init does various important housecleaning activities including maintaining a processes for each terminal port (tty)
        - Getty then executes the login program on that tty
        - Login gets username/password from the user, reads /etc/password, computes hash(salt+password) and compares
        - If login successful, login will spawn a shell process for the user
        - Shell and all its children run with that users privileges
        - Distinguishing users
            + When a user logs in, they supply a password which is checked against a password list
            + In UNIX, passwords stored in a file /etc/password
                * What is in this file?
            + Naive aproach: File with everyones password in it
                * What if compromised?
            + Better: Keep a file with hash(password)
                * One way hash function makes it hard to get from has(password) to password but easy to go passwords to hash(password)
                * Now can distribute the password file in plain text and passwords not revealed
    
    User Proceses:
        - OS will keep amintain memory protection (even amongst processes belonging to some user)
        - OS will also check file permissions for all the process attempts to access/create
    
    Domain per Process:
        - Good for programmer to be able to limit the protection doamin of a process to the minimal set necessary to accomplish a task
            + Why do I have to give every process I run my full access rights
            + Trojan horses?
        - Even within a process, the rights necessary may vary over the lifetime of the process
            + If only need certain privalages to initalize, why keep them for the entire life of the process when they can be exploited later
    
    limit/ulimit/getrlimit/setrlimit:
        - limit resource usage of a process and its decendants
        - Example limits
            + Limit data segment/heap/stack
            + Limit amount of address space mapped (VM limit)
            + Limit max CPU Time
            + Limit size of created files and number of files
            + Limit max core file size
        - Each decendant gets t oreach the limit, not cumulative: can still exced with lots of children
        - Soft/Hard limits
            + ANy user can decrease or increase up to hard limit
            + Only root can raise hard limits
        - Other limits
            + Quota: Allows limiting users consumption of hard disk space
            + Chroot: makes a specified directory the root of a processes file system such that it cannot accesss the rest of the file system
            + Free BSD has "jail" for confining root to a subset of special privalages

    Protection Domain:
        - All the possible entities to whom we would like to grant/deny rights
        - Associate with each entity a "protection domain"
        - Define a protection domain as a collection of access rights to specified objects

    Typical Domain Grandularities:
        - One domain for OS; One domain for User 
        - One domain per user
        - Domain per process
        - Domain per procedure
        - ...

    Types of access:
        - The possible types of access depentd on the resource
            + CPUs can be executed upon
            + File can be read/written/executed
            + Directories can be read/inserted into/deleted from/trversed without displayingall
            + Tape drives can be read/written/rewound

    Access Matrix
        - Now weve figured out all the objects we want to protect, the types of access we might want to grant and the entities to whom we will grant them (protection domains)
        - Result = Access Matrix
            + Rows of matrix can be domains
                * Regardless of grandularity of domain
                * if domain per user then row per user
            + Columns are objects or resource
            + Values at entry (i,j) says rights domain i has to object j

        +----+------------+------+---------+---------+
        |    |     F1     | F2   | F3      | printer | Object
        +----+------------+------+---------+---------+
        | D1 |    read    |      |  read   |         |
        +----+------------+------+---------+---------+
        | D2 |            |      |         |  print  |
        +----+------------+------+---------+---------+
        | D3 |            | read | execute |         |
        +----+------------+------+---------+---------+
        | D4 | read/write |      |  read   |         |
        +----+------------+------+---------+---------+
        Domain

    Finer Grandularity Good?
        - More expresive ability to specify certain policies
        - Mechanism vs Policies: if you dont have a way to check something then can't possibly 
    
    Finer Grandularity Bad?
        - Full access matrix is huge and unweildy (also often filled with 0s)
        - Compress in several ways 
            * Roles/catagories to represent large groups of users
            * Capabilities vs access control lists
            * Store access rights to one set of objects differently from another

    Implementation of Access Matricies:
        - 2D array - how hard can that be?
        - Well its not hard but its big and filled with lots of 0s
            + If most of the domains have permissions for only a few objects then there is lots of wasted space
            + Avoid this by choppinf up the access matrix and compressing
        - OS may also chose to divide up into logical sections 
          (ie. all protection info realted to files in one place and all protection info related to users in another)
        - Also compression from domain = groups of users
        - Also compression from inheritance

    Access Control List:
        - Chop matrix into columns and dont list doimains that have no access
        - With each object store the list of domains that can access it and in what ways
            + like a guest list
        - A domain not present on the list has no access rights
            + lots of compression that way
        - Can set a default set of rights to an object and then only need to enter exeptions to the default

    Capabilities:
        - Chop access matrix into rows and dont list objects which you have no rights
            + same benifit of compression
        - With each new domain store the list of objects it can access and in what ways
            + like key Chain
        - Principal must not be able to forge or change a capability
            + Typical stsem stores on behalf of the principal 

    Access Lists vs Capabilities:
        - With access lists, harder to ask the question what does this principal have access to
            + With capabilities, all info stored with principals account so for example,
              when firing someone easier to find all capabilities in one place than to search through access control lists for all objetcs
        - With capabilities, haerder to ask the question, who has access to this object
            + With access control lists, list of all rights held with the object
        - On access, time to validate a capability vs time to search through the ACL
        - Overhead of generating a capability or deciding whether a capability should be granted

    Rights to the access matrix?
        - In addition to object in the matrix, we can also think about rights to the matrix itself
            + Who can add rights to an entry? 
            + Who can switch which domain is active?
            + Who can add domains?
        - Additional rights
            + Copy right: allow copying of rights to other domains
            + Transfer: migrate rights from one domain to another (different than copying)
            + Owner right: addition of new rights or removal of rights
            + Switch right: ability to switch to a domain, consider domains as object

    Access Lists in Unix FS:
        - Unix FS usually contain access lists with each file 
        - Not very extensive access lists though!
            + Usually just able to specify read, write and execute rights for three groups: user, group and world
        - Can imagine more extensive access list information than this?
            + Pro: more flexible
            + Con: more storage

    More extensive mechanisms:
        - More extensive list of possible rights?
            + Larger list of possible rights to files (not just read/write/execute)
        - Finer granularity control of who accesses?
        - Allow list of users rather than user/group/all
        - Finer grain mechanism allows policies that better match need to know principle

    Hydra:
        - Multiprocessor OS from CMU 1974
        - Extremely fine grained and flexible protection system 
        - Used capabilities
        - Early object-oriented system  with OS support for objects
        - Extensible security system 
            + Users could define new types of objects to be protected


    Hydra Objects:
        - Each object has with it a collection of access rights
            + Manipulated by OS so unforgeable
            + Very early OOP concepts
            + Each object defined by data, operations that can be applied and collection of access rights to it
        - Kernel provided operations for the definition of new types of object and associated rights

    Hydra procedures:
        - Each procedure has its code and a list of caller independent capabilities and caller dependent capabilities (holes)
        - Local Name Space (LNS) 
            + When call a procedure fill in holes with your own current capabilities and gain the caller independent capabilities to form a current set of capabilities
        - Process = stack not just of procedures but also of capabilities!!
        - Great flexibility!
            + Each procedure can upgrade rights for just that procedure and also base access on right of caller

    Hydra Pros and Cons:
        - Very flexible system 
            + Implement need to know principle to level of every object and every procedure!
        - Requires domain switch for every procedure call and access rights for each object
            + GOOD: Each procedure has only rights required
            + BAD: Expensive to check constantly
                * OS trap per procedure call
        - Modern OS support for protection not this extensive
            + As we have extra performance and security more of a worry...

    Protection vs Security:
        - So far we have been dealing with protection or prevention 
            + The ability to say yes or no to certain accesses
        - Protection deals with internal access controls
            + Users must log in
            + Access to resources tracked at certain granularity
            + Access is granted by way of access list or capability
        - Security on the other hand deals more with internal access controls 
